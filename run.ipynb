{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 3. InfoExplorers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e42b070e34804c0e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:03.493266200Z",
     "start_time": "2023-12-14T13:18:03.410973700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import heapq\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0751a99dd67d9a7"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# Constant paths\n",
    "DATA_FOLDER = 'data/'\n",
    "PICKLES_FOLDER = 'pickles/'\n",
    "WIKILITE_FOLDER = DATA_FOLDER + 'wiki_lite/'\n",
    "SUBMISSIONS_FOLDER = 'submissions/'\n",
    "CORRECTED_FOLDER = 'corrected/'\n",
    "\n",
    "# Create folders if they don't exist\n",
    "if not os.path.exists(PICKLES_FOLDER):\n",
    "    os.makedirs(PICKLES_FOLDER)\n",
    "\n",
    "if not os.path.exists(SUBMISSIONS_FOLDER):\n",
    "    os.makedirs(SUBMISSIONS_FOLDER)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:53:31.658095100Z",
     "start_time": "2023-12-14T13:53:31.626563300Z"
    }
   },
   "id": "18f1bb4a3fc83cd6"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Train data loading\n",
    "try:\n",
    "    train_df = pd.read_pickle(PICKLES_FOLDER + 'train_df.pkl')\n",
    "except:\n",
    "    train_df = pd.read_csv(DATA_FOLDER + 'train.csv')\n",
    "    train_df.to_pickle(PICKLES_FOLDER + 'train_df.pkl')\n",
    "\n",
    "# Test data loading  \n",
    "try:\n",
    "    test_df = pd.read_pickle(PICKLES_FOLDER + 'test_df.pkl')\n",
    "except:\n",
    "    test_df = pd.read_csv(DATA_FOLDER + 'test.csv')\n",
    "    test_df.to_pickle(PICKLES_FOLDER + 'test_df.pkl')\n",
    "\n",
    "# Redirects loading\n",
    "try:\n",
    "    enwinki_redirects = pd.read_pickle(PICKLES_FOLDER + 'enwiki_redirects.pkl')\n",
    "except:\n",
    "    enwinki_redirects = pd.read_csv(WIKILITE_FOLDER + 'enwiki_redirects.tsv', names=['en_title', 'en_redirect_title'],\n",
    "                                    sep='\\t')\n",
    "    enwinki_redirects.to_pickle(PICKLES_FOLDER + 'enwiki_redirects.pkl')\n",
    "\n",
    "# Aliases loading  \n",
    "try:\n",
    "    item_aliases = pd.read_pickle(PICKLES_FOLDER + 'item_aliases.pkl')\n",
    "except:\n",
    "    item_aliases = pd.read_csv(WIKILITE_FOLDER + 'item_aliases.csv')\n",
    "    item_aliases.to_pickle(PICKLES_FOLDER + 'item_aliases.pkl')\n",
    "\n",
    "# Properties loading\n",
    "try:\n",
    "    properties = pd.read_pickle(PICKLES_FOLDER + 'property.pkl')\n",
    "except:\n",
    "    properties = pd.read_csv(WIKILITE_FOLDER + 'property.csv')\n",
    "    properties.to_pickle(PICKLES_FOLDER + 'property.pkl')\n",
    "\n",
    "# Statements loading\n",
    "try:\n",
    "    statements = pd.read_pickle(PICKLES_FOLDER + 'statements.pkl')\n",
    "except:\n",
    "    statements = pd.read_csv(WIKILITE_FOLDER + 'statements.csv')\n",
    "    statements.to_pickle(PICKLES_FOLDER + 'statements.pkl')\n",
    "\n",
    "# Wiki items loading \n",
    "try:\n",
    "    wiki_items = pd.read_pickle(PICKLES_FOLDER + 'wiki_items.pkl')\n",
    "except:\n",
    "    wiki_items = pd.read_csv(WIKILITE_FOLDER + 'wiki_items.csv')\n",
    "    wiki_items.to_pickle(PICKLES_FOLDER + 'wiki_items.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:13.200196800Z",
     "start_time": "2023-12-14T13:18:03.432263800Z"
    }
   },
   "id": "8b9e6f0ee2813fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Using existing datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74aa5a0e772b9c9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f73f63d272f417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merging `item_aliases` and `wiki_items` on `item_id` to get the `wikipedia_title` for each `en_alias`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b64eefea3ce0942"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "   item_id  en_label                                 en_description  \\\n0        1  Universe             totality of space and all contents   \n1        1  Universe             totality of space and all contents   \n2        1  Universe             totality of space and all contents   \n3        1  Universe             totality of space and all contents   \n4        2     Earth  third planet from the Sun in the Solar System   \n\n  wikipedia_title      en_alias  \n0        Universe  Our Universe  \n1        Universe  The Universe  \n2        Universe    The Cosmos  \n3        Universe        cosmos  \n4           Earth   Blue Planet  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_id</th>\n      <th>en_label</th>\n      <th>en_description</th>\n      <th>wikipedia_title</th>\n      <th>en_alias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>Our Universe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>The Universe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>The Cosmos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>cosmos</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Earth</td>\n      <td>third planet from the Sun in the Solar System</td>\n      <td>Earth</td>\n      <td>Blue Planet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_wiki_items = wiki_items.merge(item_aliases, how='left', on='item_id')\n",
    "merged_wiki_items.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:14.819007500Z",
     "start_time": "2023-12-14T13:18:13.201261600Z"
    }
   },
   "id": "bd280c1982f55afb"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Copying the dataframes to modify them\n",
    "test_df_mod = test_df.copy(deep=True)\n",
    "train_df_mod = train_df.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:14.833560100Z",
     "start_time": "2023-12-14T13:18:14.818008100Z"
    }
   },
   "id": "82965c83ebbb3015"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Beginning of the URL to wikipedia\n",
    "URL = 'http://en.wikipedia.org/wiki/'\n",
    "LEN_URL = len(URL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:35:52.270369Z",
     "start_time": "2023-12-14T13:35:52.258117800Z"
    }
   },
   "id": "2cb2450d10518b95"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# We need to lowercase the tokens to avoid problems with case in future steps\n",
    "train_df_mod['full_mention_lower'] = train_df_mod['full_mention'].str.lower()\n",
    "\n",
    "# We only keep the tokens that have a wiki_url\n",
    "train_df_mod = train_df_mod[train_df_mod['wiki_url'].notnull() & (train_df_mod['wiki_url'] != '--NME--')]\n",
    "\n",
    "# We also lowercase the tokens in the test data for future steps\n",
    "test_df_mod['full_mention'] = test_df_mod['full_mention'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:14.903415100Z",
     "start_time": "2023-12-14T13:18:14.836093200Z"
    }
   },
   "id": "7249692acea2e0eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform:\n",
    "- `en_alias` and `wikipedia_title` in `merged_wiki_items`\n",
    "- `wikipedia_title` in `wiki_items`\n",
    "- `en_title` in `redirects`\n",
    "\n",
    "to lowercase to avoid problems with case in future steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd5f505f0444795b"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "merged_wiki_items['en_alias_lower'] = merged_wiki_items['en_alias'].str.lower()\n",
    "merged_wiki_items['wikipedia_title_lower'] = merged_wiki_items['wikipedia_title'].str.lower()\n",
    "\n",
    "wiki_items['wikipedia_title_lower'] = wiki_items['wikipedia_title'].str.lower()\n",
    "\n",
    "enwinki_redirects['en_title_lower'] = enwinki_redirects['en_title'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:22.897708600Z",
     "start_time": "2023-12-14T13:18:14.875394200Z"
    }
   },
   "id": "7d96a359b740d910"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating dictionaries for faster lookup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef8e3b286066879e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "DICT_FOLDER = PICKLES_FOLDER + 'dictionaries/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:22.910100600Z",
     "start_time": "2023-12-14T13:18:22.898838800Z"
    }
   },
   "id": "cf3e7bcd5309c070"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "try:\n",
    "    aliases_dict = pickle.load(open(DICT_FOLDER + 'aliases_dict.pkl', 'rb'))\n",
    "except:\n",
    "    aliases_dict = pd.Series(merged_wiki_items['wikipedia_title'].values,\n",
    "                             index=merged_wiki_items['en_alias_lower']).to_dict()\n",
    "    pickle.dump(aliases_dict, open(DICT_FOLDER + 'aliases_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:24.063646700Z",
     "start_time": "2023-12-14T13:18:22.901178900Z"
    }
   },
   "id": "82d4af9957872fe7"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "try:\n",
    "    titles_dict = pickle.load(open(DICT_FOLDER + 'titles_dict.pkl', 'rb'))\n",
    "except:\n",
    "    titles_dict = pd.Series(wiki_items['wikipedia_title'].values, index=wiki_items['wikipedia_title_lower']).to_dict()\n",
    "    pickle.dump(titles_dict, open(DICT_FOLDER + 'titles_dict.pkl', 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:28.694308800Z",
     "start_time": "2023-12-14T13:18:24.063646700Z"
    }
   },
   "id": "e2a837f701077025"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dict = pickle.load(open(DICT_FOLDER + 'train_dict.pkl', 'rb'))\n",
    "except:\n",
    "    train_dict = pd.Series(train_df_mod['wiki_url'].values, index=train_df_mod['full_mention_lower']).to_dict()\n",
    "    pickle.dump(train_dict, open(DICT_FOLDER + 'train_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:28.703942500Z",
     "start_time": "2023-12-14T13:18:28.695308800Z"
    }
   },
   "id": "d6c486944ee1036d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "try:\n",
    "    redirects_dict = pickle.load(open(DICT_FOLDER + 'redirects_dict.pkl', 'rb'))\n",
    "except:\n",
    "    redirects_dict = pd.Series(enwinki_redirects['en_redirect_title'].values,\n",
    "                               index=enwinki_redirects['en_title_lower']).to_dict()\n",
    "    pickle.dump(redirects_dict, open(DICT_FOLDER + 'redirects_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:34.088818200Z",
     "start_time": "2023-12-14T13:18:28.699414400Z"
    }
   },
   "id": "252ec3cd41695de4"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/104890 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8ef28f18315440997c90e9af1718e17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We want to keep the count of the number of matches we find with aliases and redirects\n",
    "aliases_matching = 0\n",
    "redirects_matching = 0\n",
    "\n",
    "for index, row in tqdm(test_df_mod.iterrows(), total=test_df_mod.shape[0]):\n",
    "    if str(row['wiki_url']) == 'nan' or row['wiki_url'] != '?':\n",
    "        continue\n",
    "\n",
    "    token = row['full_mention']\n",
    "    train_url = train_dict.get(token)\n",
    "\n",
    "    if train_url is not None:\n",
    "        # We found a link in the train data and it is the true identity so we can use it\n",
    "        test_df_mod.at[index, 'wiki_url'] = train_url\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        # TODO: explain the order of the if statements in the report\n",
    "        wiki_title = titles_dict.get(token)\n",
    "\n",
    "        if wiki_title is not None:\n",
    "            aliases_matching += 1\n",
    "\n",
    "        else:\n",
    "            wiki_title = aliases_dict.get(token)\n",
    "\n",
    "            if wiki_title is not None:\n",
    "                aliases_matching += 1\n",
    "\n",
    "    if wiki_title is not None:\n",
    "        redirect_title = redirects_dict.get(wiki_title.lower())\n",
    "\n",
    "        if redirect_title is not None:\n",
    "            redirects_matching += 1\n",
    "            test_df_mod.at[index, 'wiki_url'] = URL + redirect_title.replace(' ', '_')\n",
    "\n",
    "        else:\n",
    "            test_df_mod.at[index, 'wiki_url'] = URL + wiki_title.replace(' ', '_')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:36.441622300Z",
     "start_time": "2023-12-14T13:18:34.092815800Z"
    }
   },
   "id": "8e186a6eee7eb6ba"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "2107"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aliases_matching"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:36.455881500Z",
     "start_time": "2023-12-14T13:18:36.443626200Z"
    }
   },
   "id": "416956ed3cbe7456"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "393"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirects_matching"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:36.456859800Z",
     "start_time": "2023-12-14T13:18:36.446389600Z"
    }
   },
   "id": "38d44acc9170b212"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how many tokens we found links for:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7507b3a85b5a8586"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously we had 9166 tokens without a link\n",
      "Now we have 547 tokens without a link\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previously we had {test_df[test_df['wiki_url'] == '?']['wiki_url'].count()} tokens without a link\")\n",
    "print(f\"Now we have {test_df_mod[test_df_mod['wiki_url'] == '?']['wiki_url'].count()} tokens without a link\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:36.505079400Z",
     "start_time": "2023-12-14T13:18:36.453839100Z"
    }
   },
   "id": "c3e735d9fe28d308"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating submission file for second part"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2f5746f1b4d3123"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "result_first_part = test_df_mod[['id', 'wiki_url']]\n",
    "result_first_part.loc[:, 'wiki_url'] = result_first_part['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http') or str(x) == '?') else x)\n",
    "# result_first_part.loc[:, 'wiki_url'] = result_first_part['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http')) else x)\n",
    "\n",
    "# name = 'tr_title_alias_dict_redirects_url'\n",
    "# \n",
    "# result_first_part.to_csv(SUBMISSIONS_FOLDER + name + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:18:36.507053700Z",
     "start_time": "2023-12-14T13:18:36.467453300Z"
    }
   },
   "id": "df485ff6287f478b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f374ebae8b86d158"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def get_connection(only_train=False):\n",
    "    \"\"\"\n",
    "    Function retrieving the connection between the wiki_items from the statements.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    only_train: bool, optional (default=False)\n",
    "        If True, only the connections between the items in the train data are returned.\n",
    "    \n",
    "    Returns\n",
    "    -------  \n",
    "    connection: pd.DataFrame\n",
    "        DataFrame containing the connections between the wiki_items.\n",
    "    \"\"\"\n",
    "    \n",
    "    connection = statements\n",
    "    if only_train:\n",
    "        list_of_ids = pd.read_csv('train_wiki.csv')  # obtain by running get_wiki.ipynb\n",
    "        list_of_ids = list_of_ids.item_id.tolist()\n",
    "        all_ids = set(connection['source_item_id'].unique()).union(set(connection['target_item_id'].unique()))\n",
    "\n",
    "        filtered_ids = set(list_of_ids).intersection(all_ids)\n",
    "\n",
    "        connection = connection[\n",
    "            connection['source_item_id'].isin(filtered_ids) | connection['target_item_id'].isin(filtered_ids)]\n",
    "    return connection\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:19:59.695330200Z",
     "start_time": "2023-12-14T13:19:59.682699200Z"
    }
   },
   "id": "f2d633317a1327cd"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def add_edges(G, connection, progress=True):\n",
    "    \"\"\"\n",
    "    Function adding the edges to the graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G: nx.Graph\n",
    "        Graph to which the edges are added.\n",
    "    connection: pd.DataFrame\n",
    "        DataFrame containing the connections between the wiki_items.\n",
    "    progress: bool, optional (default=True)\n",
    "        If True, a progress bar is shown.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    G: nx.Graph\n",
    "        Graph with the added edges. \n",
    "    \"\"\"\n",
    "    \n",
    "    if progress:\n",
    "        for source_item_id, _, target_item_id in tqdm(connection.iloc, total=len(connection)):\n",
    "            G.add_edge(source_item_id, target_item_id)\n",
    "    else:\n",
    "        G = nx.from_pandas_edgelist(connection, 'source_item_id', 'target_item_id', create_using=nx.Graph)\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:20:00.359559500Z",
     "start_time": "2023-12-14T13:20:00.354780200Z"
    }
   },
   "id": "e80f9d6941dae5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "connection = get_connection()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:20:01.377883100Z",
     "start_time": "2023-12-14T13:20:01.355001200Z"
    }
   },
   "id": "e7715b6118132c71"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/26903188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c51e1d2b21b427bb4a8f2aaedfd8bde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.Graph()  # Undirected Graph\n",
    "G = add_edges(G, connection, progress=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:27:56.499620400Z",
     "start_time": "2023-12-14T13:20:04.540821900Z"
    }
   },
   "id": "110f7efab511391e"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4906271\n",
      "Number of edges: 24528246\n"
     ]
    }
   ],
   "source": [
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:27:58.090942500Z",
     "start_time": "2023-12-14T13:27:56.500682900Z"
    }
   },
   "id": "789a31308f2cfcff"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "pickle_filename = \"graph_undirected_full.pkl\"\n",
    "\n",
    "# Save graph pickle to the file\n",
    "with open(PICKLES_FOLDER + pickle_filename, 'wb') as pickle_file:\n",
    "    pickle.dump(G, pickle_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:28:13.974724100Z",
     "start_time": "2023-12-14T13:27:58.093891600Z"
    }
   },
   "id": "faed01c6c265602e"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Specify the filename for the pickle file\n",
    "pickle_filename = \"graph_undirected_full.pkl\"\n",
    "\n",
    "# Load the graph from the pickle file\n",
    "with open(PICKLES_FOLDER + pickle_filename, 'rb') as pickle_file:\n",
    "    G = pickle.load(pickle_file).to_undirected()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:30:31.491183700Z",
     "start_time": "2023-12-14T13:28:13.976725300Z"
    }
   },
   "id": "a980c25f84084466"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN             NaN   \n1            1                        CRICKET        NaN             NaN   \n2            2                              -        NaN             NaN   \n3            3                 LEICESTERSHIRE          B  LEICESTERSHIRE   \n4            4                           TAKE        NaN             NaN   \n...        ...                            ...        ...             ...   \n104885  104885                        brother        NaN             NaN   \n104886  104886                              ,        NaN             NaN   \n104887  104887                          Bobby          B           Bobby   \n104888  104888                              .        NaN             NaN   \n104889  104889                            NaN        NaN             NaN   \n\n       wiki_url_x                                         wiki_url_y  \n0             NaN                                          NOT_FOUND  \n1             NaN                                          NOT_FOUND  \n2             NaN                                          NOT_FOUND  \n3               ?  http://en.wikipedia.org/wiki/Leicestershire_Co...  \n4             NaN                                          NOT_FOUND  \n...           ...                                                ...  \n104885        NaN                                          NOT_FOUND  \n104886        NaN                                          NOT_FOUND  \n104887          ?                 http://en.wikipedia.org/wiki/Bobby  \n104888        NaN                                          NOT_FOUND  \n104889        NaN                                          NOT_FOUND  \n\n[104890 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>full_mention</th>\n      <th>wiki_url_x</th>\n      <th>wiki_url_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>LEICESTERSHIRE</td>\n      <td>?</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>Bobby</td>\n      <td>?</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking the result from the first part\n",
    "partial = result_first_part\n",
    "\n",
    "testdf = test_df.copy(deep=True)\n",
    "testdf = testdf.merge(partial, on='id')\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:55:36.089392400Z",
     "start_time": "2023-12-14T13:55:36.064712Z"
    }
   },
   "id": "c74cc83c41c2746b"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "         id                    en_redirect_title\n0       795   Universities in the United Kingdom\n1       975   Universities in the United Kingdom\n2      4396                           David Barr\n3      4402                     Michael Sullivan\n4      4552  United States Amateur Championships\n..      ...                                  ...\n342  103998                    Predrag Mijatović\n343  104041                    Predrag Mijatović\n344  104124                            Mijatović\n345  104310                        De Graafschap\n346  104877                  1966 FIFA World Cup\n\n[347 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>en_redirect_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>795</td>\n      <td>Universities in the United Kingdom</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>975</td>\n      <td>Universities in the United Kingdom</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4396</td>\n      <td>David Barr</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4402</td>\n      <td>Michael Sullivan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4552</td>\n      <td>United States Amateur Championships</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>103998</td>\n      <td>Predrag Mijatović</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>104041</td>\n      <td>Predrag Mijatović</td>\n    </tr>\n    <tr>\n      <th>344</th>\n      <td>104124</td>\n      <td>Mijatović</td>\n    </tr>\n    <tr>\n      <th>345</th>\n      <td>104310</td>\n      <td>De Graafschap</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>104877</td>\n      <td>1966 FIFA World Cup</td>\n    </tr>\n  </tbody>\n</table>\n<p>347 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "redirect = enwinki_redirects\n",
    "temp = testdf[partial.wiki_url == \"?\"].merge(redirect, left_on='full_mention', right_on='en_title')[['id', 'en_redirect_title']]\n",
    "\n",
    "display(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:33:18.409451600Z",
     "start_time": "2023-12-14T13:33:14.934951800Z"
    }
   },
   "id": "6070da4a30bc3db4"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN   \n1            1                        CRICKET        NaN   \n2            2                              -        NaN   \n3            3                 LEICESTERSHIRE          B   \n4            4                           TAKE        NaN   \n...        ...                            ...        ...   \n104885  104885                        brother        NaN   \n104886  104886                              ,        NaN   \n104887  104887                          Bobby          B   \n104888  104888                              .        NaN   \n104889  104889                            NaN        NaN   \n\n                                                 wiki_url en_redirect_title  \n0                                               NOT_FOUND               NaN  \n1                                               NOT_FOUND               NaN  \n2                                               NOT_FOUND               NaN  \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...    LEICESTERSHIRE  \n4                                               NOT_FOUND               NaN  \n...                                                   ...               ...  \n104885                                          NOT_FOUND               NaN  \n104886                                          NOT_FOUND               NaN  \n104887                 http://en.wikipedia.org/wiki/Bobby             Bobby  \n104888                                          NOT_FOUND               NaN  \n104889                                          NOT_FOUND               NaN  \n\n[104890 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>wiki_url</th>\n      <th>en_redirect_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n      <td>LEICESTERSHIRE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n      <td>Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdf = testdf.merge(temp, on='id', how='left')\n",
    "testdf['en_redirect_title'] = testdf.apply(lambda row: row.full_mention if pd.isna(row['en_redirect_title']) else row['en_redirect_title'], axis=1)\n",
    "\n",
    "testdf = testdf.drop(columns=['wiki_url_x', 'full_mention'])\n",
    "testdf.rename(columns={'wiki_url_y': 'wiki_url'}, inplace=True)\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:55:40.882477100Z",
     "start_time": "2023-12-14T13:55:40.129336500Z"
    }
   },
   "id": "4cd6d5c1e8498977"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "          item_id                 en_label          wikipedia_title\n0               1                 Universe                 Universe\n1               2                    Earth                    Earth\n2               3                     life                     Life\n3               4                    death                    Death\n4               5                    human                    Human\n...           ...                      ...                      ...\n5216231  77042017                  HR 4523                HD 102365\n5216232  77043280         Charlie Johnston        Charlie Johnstone\n5216233  77231860               Aldo Rossi    Aldo Rossi (musician)\n5216234  77240068  Ebenezer Baptist Church  Ebenezer Baptist Church\n5216235  77242291                New Court                New Court\n\n[5216236 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_id</th>\n      <th>en_label</th>\n      <th>wikipedia_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>Universe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Earth</td>\n      <td>Earth</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>life</td>\n      <td>Life</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>death</td>\n      <td>Death</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>human</td>\n      <td>Human</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5216231</th>\n      <td>77042017</td>\n      <td>HR 4523</td>\n      <td>HD 102365</td>\n    </tr>\n    <tr>\n      <th>5216232</th>\n      <td>77043280</td>\n      <td>Charlie Johnston</td>\n      <td>Charlie Johnstone</td>\n    </tr>\n    <tr>\n      <th>5216233</th>\n      <td>77231860</td>\n      <td>Aldo Rossi</td>\n      <td>Aldo Rossi (musician)</td>\n    </tr>\n    <tr>\n      <th>5216234</th>\n      <td>77240068</td>\n      <td>Ebenezer Baptist Church</td>\n      <td>Ebenezer Baptist Church</td>\n    </tr>\n    <tr>\n      <th>5216235</th>\n      <td>77242291</td>\n      <td>New Court</td>\n      <td>New Court</td>\n    </tr>\n  </tbody>\n</table>\n<p>5216236 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_item = wiki_items[['item_id', 'en_label', 'wikipedia_title']]\n",
    "# wiki_item = wiki_item[wiki_item['item_id'].isin(filtered_ids)]\n",
    "\n",
    "# wiki_item['en_label'] = wiki_item['en_label'] + \" \" + wiki_item['wikipedia_title']\n",
    "col_ = wiki_item.wikipedia_title.str.lower().str\n",
    "lower_case_wiki_item_titles = wiki_item.wikipedia_title.str.lower().str\n",
    "display(wiki_item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:34:20.787779800Z",
     "start_time": "2023-12-14T13:34:20.040525700Z"
    }
   },
   "id": "b7c7606df31d5ffe"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN   \n1            1                        CRICKET        NaN   \n2            2                              -        NaN   \n3            3                 LEICESTERSHIRE          B   \n4            4                           TAKE        NaN   \n...        ...                            ...        ...   \n104885  104885                        brother        NaN   \n104886  104886                              ,        NaN   \n104887  104887                          Bobby          B   \n104888  104888                              .        NaN   \n104889  104889                            NaN        NaN   \n\n                                                 wiki_url en_redirect_title  \\\n0                                               NOT_FOUND               NaN   \n1                                               NOT_FOUND               NaN   \n2                                               NOT_FOUND               NaN   \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...    LEICESTERSHIRE   \n4                                               NOT_FOUND               NaN   \n...                                                   ...               ...   \n104885                                          NOT_FOUND               NaN   \n104886                                          NOT_FOUND               NaN   \n104887                 http://en.wikipedia.org/wiki/Bobby             Bobby   \n104888                                          NOT_FOUND               NaN   \n104889                                          NOT_FOUND               NaN   \n\n          item_id  \n0             NaN  \n1             NaN  \n2             NaN  \n3       3229147.0  \n4             NaN  \n...           ...  \n104885        NaN  \n104886        NaN  \n104887   289262.0  \n104888        NaN  \n104889        NaN  \n\n[104890 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>wiki_url</th>\n      <th>en_redirect_title</th>\n      <th>item_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n      <td>LEICESTERSHIRE</td>\n      <td>3229147.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n      <td>Bobby</td>\n      <td>289262.0</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdf['wiki_title'] = testdf.wiki_url.str[LEN_URL:].str.replace('_', ' ')\n",
    "testdf = testdf.merge(wiki_item, left_on='wiki_title', right_on='wikipedia_title', how='left').drop(\n",
    "    columns=['wikipedia_title', 'en_label', 'wiki_title'])\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:55:53.362210300Z",
     "start_time": "2023-12-14T13:55:51.535061700Z"
    }
   },
   "id": "caf97f2b1b17be22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_dist(certain_list, candidate, fill_na=9999):\n",
    "    \"\"\"\n",
    "    Function calculating the distance between the full mention and a candidate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    certain_list: list\n",
    "        List containing the already assigned entities.\n",
    "    candidate: float\n",
    "        Candidate id.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distance: float\n",
    "        Distance between the full mention and the candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance_list = []\n",
    "    subset_certain = list(certain_list)\n",
    "    random.shuffle(subset_certain)\n",
    "    subset_size = 10\n",
    "    subset_certain = subset_certain[:subset_size]\n",
    "    \n",
    "    for assigned_entity in subset_certain:\n",
    "        try:\n",
    "            shortest = nx.shortest_path_length(G, source=assigned_entity, target=candidate)\n",
    "            distance_list.append(shortest)\n",
    "        except:\n",
    "            distance_list.append(fill_na)\n",
    "            \n",
    "    return sum(distance_list) / subset_size if distance_list else fill_na"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a769a09c2239499"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def get_all_dist(filtered_ls, full_mention_train):\n",
    "    \"\"\"\n",
    "    Function calculating the distance between the full mention and all the candidates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filtered_ls: pd.DataFrame\n",
    "        DataFrame containing the candidates.\n",
    "    full_mention_train: list\n",
    "        List of the already assigned entities.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distances: list\n",
    "        List containing the distances between the full mention and all the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for candidate, _, title in filtered_ls.iloc:\n",
    "        distances.append((title, candidate, get_dist(full_mention_train, candidate)))\n",
    "        \n",
    "    return distances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:40:47.570829500Z",
     "start_time": "2023-12-14T13:40:47.567321300Z"
    }
   },
   "id": "b3d5f1e3fa796f46"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def get_info(current_document):\n",
    "    \"\"\"\n",
    "    Function retrieving the information from the current document.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_document: list\n",
    "        List containing all the rows of the current document.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    found: pd.DataFrame\n",
    "        DataFrame containing the rows of the current document that have a link.\n",
    "    not_found: pd.DataFrame\n",
    "        DataFrame containing the rows of the current document that have to be assigned.\n",
    "    item_id_train: set\n",
    "        Set containing the ids of the entities that have a link.\n",
    "    full_mention_found: dict\n",
    "        Dictionary containing the full mentions that have a link and the corresponding link.\n",
    "    mention_test: set\n",
    "        Set containing the full mentions that have to be assigned.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(current_document):\n",
    "        document_df = pd.DataFrame(current_document)\n",
    "        full_mention_ = document_df[document_df['wiki_url'] != 'NOT_FOUND']\n",
    "        found = full_mention_[full_mention_['wiki_url'] != '?']\n",
    "        item_id_train = set(found.item_id.tolist())\n",
    "        full_mention_found = dict(zip(found['full_mention'].str.lower(), found['wiki_url']))\n",
    "\n",
    "        not_found = full_mention_[full_mention_['wiki_url'] == '?'].copy()\n",
    "        not_found.full_mention = not_found.full_mention.str.lower()\n",
    "        mention_test = set(not_found.full_mention.tolist())\n",
    "        \n",
    "        return found, not_found, item_id_train, full_mention_found, mention_test\n",
    "    \n",
    "    else:\n",
    "        return None, None, None, None, None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T13:43:01.392933500Z",
     "start_time": "2023-12-14T13:43:01.389883200Z"
    }
   },
   "id": "8cf28f991162537d"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def find_doc_range(df):\n",
    "    \"\"\"\n",
    "    Function returning a zipped tuple of end indexes and start indexes of documents.\n",
    "    \"\"\"\n",
    "    # TODO doc\n",
    "    def check_docstart(row):\n",
    "        \"\"\"\n",
    "        True if the current row is the row where the document starts.\n",
    "        \"\"\"\n",
    "        if pd.notnull(row['token']) and 'DOCSTART' in row['token']:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    data = df.copy()\n",
    "    data['docstart_id'] = df.apply(check_docstart, axis=1)\n",
    "    start_ids = data[data['docstart_id']]['id'].values\n",
    "    print(len(start_ids))\n",
    "    end_ids = start_ids[1:] - 1\n",
    "    end_ids = np.append(end_ids,len(df))\n",
    "    docs_range = zip(start_ids, end_ids)\n",
    "    \n",
    "    return docs_range"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:09:04.083038900Z",
     "start_time": "2023-12-14T14:09:04.051564400Z"
    }
   },
   "id": "67fa300bff9ad2e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_document_findings(document_dataframe):\n",
    "    \"\"\"\n",
    "    Function retrieving and separating entities depending on weather they are associated with a found url or not.\n",
    "    \"\"\"\n",
    "    #TODO doc.\n",
    "    document = document_dataframe[document_dataframe['wiki_url'] != 'NOT_FOUND']\n",
    "    found_links = document[document['wiki_url'] != '?']\n",
    "    not_found_links = document[document['wiki_url'] == '?']\n",
    "    return document, found_links, not_found_links"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39c20eb8f5859293"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_equal_string_candidates(mention, full_mention_found):\n",
    "    #TODO doc.\n",
    "    return set([url for found_mention, url in full_mention_found.items() if mention in found_mention])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42657415b7e35c00"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3954033972.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    def find\u001B[0m\n\u001B[0m            ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def find_substring_candidates(mention, saved_candidates):\n",
    "    #TODO make sure all dataframe have a lower case version for comparaison and use it.\n",
    "    #TODO doc.\n",
    "    if mention not in saved_candidates.keys(): \n",
    "        candidates = wiki_item.loc[lower_case_wiki_item_titles.contains(r'\\b{}\\b'.format(mention), na=False)]\n",
    "        saved_candidates[mention] = candidates\n",
    "        return candidates\n",
    "    else:\n",
    "        return saved_candidates[uncertain_word]\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T22:00:22.365458Z",
     "start_time": "2023-12-14T22:00:22.364847Z"
    }
   },
   "id": "5a7fb74273865f75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_best_candidate(candidates_for_mention, found_ids):\n",
    "    \"\"\"\n",
    "    Function returning the best candidates for the mention.\n",
    "    \"\"\"\n",
    "    distances = get_all_dist(candidates_for_mention, found_ids)\n",
    "    obtained_distances = len(distances)\n",
    "    if obtained_distances == 1:\n",
    "        title, _, _ = distances[0]\n",
    "        return URL + title.replace(' ', '_')\n",
    "    elif obtained_distances >=1:\n",
    "        first_candidate, second_candidate = heapq.nsmallest(2, distances,key=lambda x: x[-1])       \n",
    "        if first_candidate[-1] < second_candidate[-1]:\n",
    "            title, _, _ = first_candidate\n",
    "            return URL + title.replace(' ', '_')\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f32f033ad499a985"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the global constants and variables used for graph decisions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d866ca71704049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_CANDIDATES = 50\n",
    "MAX_FETCHING_TRIES = 3\n",
    "SEED = 42\n",
    "OLD_SIZE = np.inf\n",
    "current_document = []\n",
    "saved_candidates = {}\n",
    "\n",
    "random.seed(SEED)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7036f038c34356b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testdf['full_mention'] = testdf['full_mention'].str.lower()\n",
    "docs_range = find_doc_range(testdf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5859e944deadf52"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    },
    {
     "data": {
      "text/plain": "      id                          token    full_mention  \\\n0      0  -DOCSTART- (947testa CRICKET)             NaN   \n1      1                        CRICKET             NaN   \n2      2                              -             NaN   \n3      3                 LEICESTERSHIRE  LEICESTERSHIRE   \n4      4                           TAKE             NaN   \n..   ...                            ...             ...   \n451  451                        reached             NaN   \n452  452                            108             NaN   \n453  453                            for             NaN   \n454  454                          three             NaN   \n455  455                              .             NaN   \n\n                                              wiki_url  \n0                                            NOT_FOUND  \n1                                            NOT_FOUND  \n2                                            NOT_FOUND  \n3    http://en.wikipedia.org/wiki/Leicestershire_Co...  \n4                                            NOT_FOUND  \n..                                                 ...  \n451                                          NOT_FOUND  \n452                                          NOT_FOUND  \n453                                          NOT_FOUND  \n454                                          NOT_FOUND  \n455                                          NOT_FOUND  \n\n[456 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>LEICESTERSHIRE</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>451</th>\n      <td>451</td>\n      <td>reached</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>452</td>\n      <td>108</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>453</th>\n      <td>453</td>\n      <td>for</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>454</th>\n      <td>454</td>\n      <td>three</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>455</th>\n      <td>455</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n  </tbody>\n</table>\n<p>456 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for start_ids, end_ids in docs_range:\n",
    "    current_document = testdf.iloc[start_ids:end_ids]\n",
    "    display(current_document)\n",
    "    break #TODO remove\n",
    "    #TODO make sure that everything is lowercase when comparing strings. IN ALL FUNCTIONS.\n",
    "    \n",
    "    # We are interested in the rows that should have a link\n",
    "    current_document, found_links, not_found_links = split_document_findings(current_document)\n",
    "    \n",
    "    found_links_count = len(found_links)\n",
    "    \n",
    "    for _ in range(MAX_FETCHING_TRIES):\n",
    "        #Shuffle mentions TODO check shuffle works and does not mess up the indexes\n",
    "        random_mentions = not_found_links[['item_id','full_mention']].sample(frac=1)\n",
    "        \n",
    "        #Find candidates\n",
    "        candidates_urls = pd.DataFrame({\n",
    "            'item_id': random_mentions['item_id'],\n",
    "            'candidates_url': random_mentions.apply(lambda mention: find_equal_string_candidates(mention, dict(zip(found['full_mention'], found['wiki_url']))))\n",
    "        })\n",
    "        len_candidates_urls = candidates_urls['candidates_url'].apply(len)\n",
    "        \n",
    "        #for single candidates, retrieve the value and attribute it.\n",
    "        single_candidates = candidates_urls[len_candidates_urls == 1]\n",
    "        for idx, row in single_candidates.iterrows():\n",
    "            #TODO check indexes not messed up\n",
    "            found_links.at[row['item_id'], 'wiki_url'] = row['candidates_url'][0]\n",
    "        \n",
    "        # Update not found links\n",
    "        not_found_links = not_found_links[~not_found_links['item_id'].isin(single_candidates['item_id'])]\n",
    "\n",
    "        \n",
    "        #try finding the best for substrings\n",
    "        candidates_urls = pd.DataFrame({\n",
    "            'item_id': random_mentions['item_id'],\n",
    "            'candidates_url': random_mentions.apply(lambda mention: find_substring_candidates(mention, saved_candidates))\n",
    "        })\n",
    "        \n",
    "        # Calculate the number of candidates for each mention\n",
    "        len_candidates_urls = candidates_urls['candidates_url'].apply(len)\n",
    "        \n",
    "        # Process single candidates\n",
    "        single_candidates = candidates_urls[len_candidates_urls == 1]\n",
    "        for idx, row in single_candidates.iterrows():\n",
    "            #TODO check indexes not messed up\n",
    "            found_links.at[idx, 'wiki_url'] = row['candidates_url'][0]\n",
    "        \n",
    "        # Filter for multi candidates\n",
    "        multi_candidates = candidates_urls[(len_candidates_urls > 1) & (len_candidates_urls <= MAX_CANDIDATES)]\n",
    "        \n",
    "        # Apply find_best_candidate for each mention in multi_candidates\n",
    "        for not_found_id in multi_candidates['item_id']:\n",
    "            #TODO check indexes not messed up\n",
    "            best_candidate = find_best_candidate(multi_candidates[multi_candidates['item_id'] == not_found_id]['candidates_url'].iloc[0], found_links['item_id'])\n",
    "            \n",
    "            if best_candidate is not None:\n",
    "                found_links.at['item_id', 'wiki_url'] = best_candidate\n",
    "        \n",
    "        if found_links_count < len(found_links):\n",
    "            found_links_count = len(found_links)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "# TODO re-concat not_found and found."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:09:07.064035100Z",
     "start_time": "2023-12-14T14:09:06.419820700Z"
    }
   },
   "id": "d7d41bff18135424"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "Processing:   0%|          | 0/104890 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "781584581ca74576b608cad2b4570316"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transylvania has too many candidates 82\n",
      "given has too many candidates 1929\n",
      "serbia and montenegro has too many candidates 64\n",
      "southeast asia has too many candidates 67\n",
      "No match for mohammed idris\n",
      "No match for carroll a. campbell jr.\n",
      "No match for pirelli cables\n",
      "azerbaijani has too many candidates 140\n",
      "azerbaijani has too many candidates 140\n",
      "republika has too many candidates 103\n",
      "No match for brazilian-born\n",
      "donato has too many candidates 132\n",
      "No match for inverness thistle f.c.\n",
      "No match for inverness thistle f.c.\n",
      "fernández has too many candidates 874\n",
      "inter milan has too many candidates 58\n",
      "inter milan has too many candidates 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\AppData\\Local\\Temp\\ipykernel_3628\\3111897778.py:48: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_ls = wiki_item.loc[col_.contains(r'\\b{}\\b'.format(uncertain_word),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for labour party (uk)\n",
      "No match for london-based\n",
      "bahraini has too many candidates 123\n",
      "No match for interstates 80\n",
      "No match for warner bros.\n",
      "afc asian cup has too many candidates 124\n",
      "cuttitta seems to belong to {'http://en.wikipedia.org/wiki/Massimo_Cuttitta', 'http://en.wikipedia.org/wiki/Marcello_Cuttitta'}\n",
      "No match for 1995 world cup\n",
      "afc asian cup has too many candidates 124\n",
      "asian cup has too many candidates 171\n",
      "warne has too many candidates 52\n",
      "No match for michael divenuto\n",
      "No match for ny islanders\n",
      "No match for ny rangers\n",
      "anaheim has too many candidates 94\n",
      "sacramento has too many candidates 436\n",
      "golden state has too many candidates 83\n",
      "No match for ny jets\n",
      "green bay has too many candidates 224\n",
      "national has too many candidates 33477\n",
      "No match for uefa cups\n",
      "No match for uefa cups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\AppData\\Local\\Temp\\ipykernel_3628\\3111897778.py:48: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_ls = wiki_item.loc[col_.contains(r'\\b{}\\b'.format(uncertain_word),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for northwest (disambiguation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\AppData\\Local\\Temp\\ipykernel_3628\\3111897778.py:48: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_ls = wiki_item.loc[col_.contains(r'\\b{}\\b'.format(uncertain_word),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for salient (geography)\n",
      "tempe has too many candidates 62\n",
      "No match for cocker spaniels\n",
      "national alliance has too many candidates 83\n",
      "uganda has too many candidates 686\n",
      "uganda has too many candidates 686\n",
      "national has too many candidates 33477\n",
      "national has too many candidates 33477\n",
      "national has too many candidates 33477\n",
      "No match for oestersund\n",
      "No match for oestersund\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balanton\\AppData\\Local\\Temp\\ipykernel_3628\\3111897778.py:48: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_ls = wiki_item.loc[col_.contains(r'\\b{}\\b'.format(uncertain_word),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for united provinces (1937–50)\n",
      "No match for inverness thistle f.c.\n",
      "No match for elgin city f.c.\n",
      "No match for inverness thistle f.c.\n",
      "No match for n.ireland\n",
      "No match for n.ireland\n",
      "No match for michael divenuto\n",
      "No match for michael divenuto\n",
      "afc asian cup has too many candidates 124\n",
      "afc asian cup has too many candidates 124\n",
      "asian cup has too many candidates 171\n",
      "afc asian cup has too many candidates 124\n",
      "afc asian cup has too many candidates 124\n",
      "asian cup has too many candidates 171\n",
      "sacramento has too many candidates 436\n",
      "golden state has too many candidates 83\n",
      "sacramento has too many candidates 436\n",
      "golden state has too many candidates 83\n",
      "No match for ny islanders\n",
      "No match for ny rangers\n",
      "anaheim has too many candidates 94\n",
      "No match for ny rangers\n"
     ]
    }
   ],
   "source": [
    "id = 1000\n",
    "MAX_CANDIDATES = 50\n",
    "OLD_SIZE = np.inf\n",
    "current_document = []\n",
    "saved_candidates = {}\n",
    "\n",
    "for index in tqdm(range(len(testdf)), desc=\"Processing\", total=len(testdf)):\n",
    "    row = testdf.iloc[index]\n",
    "    current_document.append({\n",
    "        'id': row['id'],\n",
    "        'token': row['token'],\n",
    "        'full_mention': row['en_redirect_title'],\n",
    "        'wiki_url': row['wiki_url'],\n",
    "        'item_id': row['item_id']\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        if row['token'].startswith('-DOCSTART-'):\n",
    "            document_df = pd.DataFrame(current_document)\n",
    "            current_document = []\n",
    "\n",
    "            full_mention_ = document_df[document_df['wiki_url'] != 'NOT_FOUND']\n",
    "            found = full_mention_[full_mention_['wiki_url'] != '?']\n",
    "            item_id_train = set(found.item_id.tolist())\n",
    "            full_mention_found = dict(zip(found['full_mention'].str.lower(), found['wiki_url']))\n",
    "\n",
    "            not_found = full_mention_[full_mention_['wiki_url'] == '?'].copy()\n",
    "            not_found.full_mention = not_found.full_mention.str.lower()\n",
    "            mention_test = set(not_found.full_mention.tolist())\n",
    "\n",
    "            old_size = OLD_SIZE\n",
    "            new_size = len(mention_test)\n",
    "            correct_links = []\n",
    "            for tries in range(3):\n",
    "                if new_size < old_size:\n",
    "                    random_order = list(mention_test)\n",
    "                    random.shuffle(random_order)\n",
    "\n",
    "                    for uncertain_word in (random_order):\n",
    "                        correct_link = None\n",
    "                        matching_urls = set(\n",
    "                            [url for mention, url in full_mention_found.items() if uncertain_word in mention])\n",
    "                        if len(matching_urls) == 1:  # if uncertain_word is a part of the correct entity\n",
    "                            full_mention_found[uncertain_word] = list(matching_urls)[0]\n",
    "                        elif len(matching_urls) > 1:\n",
    "                            print(uncertain_word, \"seems to belong to\", matching_urls)\n",
    "                        else:\n",
    "                            filtered_ls = wiki_item.loc[col_.contains(r'\\b{}\\b'.format(uncertain_word),na=False)] if uncertain_word not in saved_candidates.keys() else \\\n",
    "                            saved_candidates[uncertain_word]\n",
    "                            saved_candidates[uncertain_word] = filtered_ls\n",
    "                            no_candidates = len(filtered_ls)\n",
    "                            if not no_candidates:\n",
    "                                print(\"No match for\", uncertain_word)\n",
    "                            elif no_candidates == 1:\n",
    "                                full_mention_found[uncertain_word] = URL + \\\n",
    "                                                                     filtered_ls.wikipedia_title.tolist()[0].replace(\n",
    "                                                                         ' ', '_')\n",
    "                                item_id_train.add(filtered_ls.item_id.tolist()[0])\n",
    "                            elif no_candidates < MAX_CANDIDATES:\n",
    "                                distances = get_all_dist(filtered_ls, item_id_train)\n",
    "                                if len(distances) > 1:\n",
    "                                    first_candidate, second_candidate = heapq.nsmallest(2, distances,\n",
    "                                                                                        key=lambda x: x[-1])\n",
    "                                    if first_candidate[-1] < second_candidate[-1]:\n",
    "                                        title, choice, _ = first_candidate\n",
    "                                        full_mention_found[\n",
    "                                            uncertain_word] = URL + title.replace(' ', '_')\n",
    "                                        item_id_train.add(choice)\n",
    "                                    else:\n",
    "                                        print(\"can not decide between\", first_candidate, \"and\", second_candidate)\n",
    "                                else:\n",
    "                                    title, choice, _ = distances[0]\n",
    "                                    full_mention_found[\n",
    "                                        uncertain_word] = URL + title.replace(' ', '_')\n",
    "                                    item_id_train.add(choice)\n",
    "\n",
    "                            else:\n",
    "                                print(uncertain_word, \"has too many candidates\", no_candidates)\n",
    "                    old_size = len(mention_test)\n",
    "                    mention_test.difference_update(full_mention_found.keys())\n",
    "                    new_size = len(mention_test)\n",
    "            if len(not_found):\n",
    "                not_found.wiki_url = not_found.full_mention.map(full_mention_found)\n",
    "                not_found.to_csv(f'{CORRECTED_FOLDER}{id}.csv', index=False)\n",
    "                id += 1\n",
    "    except Exception as e:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:02:28.332797400Z",
     "start_time": "2023-12-14T13:55:57.019120700Z"
    }
   },
   "id": "763d11b49dbff51a"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)             NaN   \n1            1                        CRICKET             NaN   \n2            2                              -             NaN   \n3            3                 LEICESTERSHIRE  LEICESTERSHIRE   \n4            4                           TAKE             NaN   \n...        ...                            ...             ...   \n104885  104885                        brother             NaN   \n104886  104886                              ,             NaN   \n104887  104887                          Bobby           Bobby   \n104888  104888                              .             NaN   \n104889  104889                            NaN             NaN   \n\n                                                 wiki_url  \n0                                               NOT_FOUND  \n1                                               NOT_FOUND  \n2                                               NOT_FOUND  \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...  \n4                                               NOT_FOUND  \n...                                                   ...  \n104885                                          NOT_FOUND  \n104886                                          NOT_FOUND  \n104887                 http://en.wikipedia.org/wiki/Bobby  \n104888                                          NOT_FOUND  \n104889                                          NOT_FOUND  \n\n[104890 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>LEICESTERSHIRE</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>Bobby</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "partial = result_first_part\n",
    "testdf = test_df[['id', 'token', 'full_mention']]\n",
    "testdf = testdf.merge(partial, on='id')\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:02:28.372251900Z",
     "start_time": "2023-12-14T14:02:28.333796400Z"
    }
   },
   "id": "2cb917ad6d680791"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "109"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_df = pd.DataFrame()\n",
    "counter = 0\n",
    "for i in range(100000):\n",
    "    try:\n",
    "        corrected_df = pd.concat([corrected_df, pd.read_csv(f'{CORRECTED_FOLDER}{i}.csv')[['id', 'wiki_url']]],\n",
    "                                 ignore_index=True)\n",
    "        counter += 1\n",
    "    except Exception as e:\n",
    "        pass\n",
    "corrected_df = corrected_df.drop_duplicates('id')\n",
    "counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:02:32.387573900Z",
     "start_time": "2023-12-14T14:02:28.364232800Z"
    }
   },
   "id": "a1b476e3b1144af7"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    },
    {
     "data": {
      "text/plain": "            id                          token    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)             NaN   \n1            1                        CRICKET             NaN   \n2            2                              -             NaN   \n3            3                 LEICESTERSHIRE  LEICESTERSHIRE   \n4            4                           TAKE             NaN   \n...        ...                            ...             ...   \n104885  104885                        brother             NaN   \n104886  104886                              ,             NaN   \n104887  104887                          Bobby           Bobby   \n104888  104888                              .             NaN   \n104889  104889                            NaN             NaN   \n\n                                                 wiki_url  \n0                                               NOT_FOUND  \n1                                               NOT_FOUND  \n2                                               NOT_FOUND  \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...  \n4                                               NOT_FOUND  \n...                                                   ...  \n104885                                          NOT_FOUND  \n104886                                          NOT_FOUND  \n104887                 http://en.wikipedia.org/wiki/Bobby  \n104888                                          NOT_FOUND  \n104889                                          NOT_FOUND  \n\n[104890 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>LEICESTERSHIRE</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>Bobby</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(testdf, corrected_df, on='id', how='left', suffixes=('_original', '_update'))\n",
    "\n",
    "merged_df['wiki_url'] = merged_df['wiki_url_update'].combine_first(merged_df['wiki_url_original'])\n",
    "\n",
    "merged_df = merged_df.drop(['wiki_url_original', 'wiki_url_update'], axis=1)\n",
    "print(len(merged_df[merged_df.wiki_url == '?']))\n",
    "merged_df.loc[merged_df['wiki_url'] == '?', 'wiki_url'] = 'NOT_FOUND'\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:02:32.475125300Z",
     "start_time": "2023-12-14T14:02:32.389562Z"
    }
   },
   "id": "70e3fd5e0128c4a9"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "merged_df[['id', 'wiki_url']].to_csv(SUBMISSIONS_FOLDER + 'submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:02:32.545148900Z",
     "start_time": "2023-12-14T14:02:32.433202400Z"
    }
   },
   "id": "f42ed16e4c816fad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
