{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 3. InfoExplorers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e42b070e34804c0e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:29.632460600Z",
     "start_time": "2023-12-17T15:34:28.478962900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import heapq\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0751a99dd67d9a7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Constant paths\n",
    "DATA_FOLDER = 'data/'\n",
    "PICKLES_FOLDER = 'pickles/'\n",
    "WIKILITE_FOLDER = DATA_FOLDER + 'wiki_lite/'\n",
    "SUBMISSIONS_FOLDER = 'submissions/'\n",
    "CORRECTED_FOLDER = 'corrected/'\n",
    "\n",
    "# Create folders if they don't exist\n",
    "if not os.path.exists(PICKLES_FOLDER):\n",
    "    os.makedirs(PICKLES_FOLDER)\n",
    "\n",
    "if not os.path.exists(SUBMISSIONS_FOLDER):\n",
    "    os.makedirs(SUBMISSIONS_FOLDER)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:29.642819200Z",
     "start_time": "2023-12-17T15:34:29.636299400Z"
    }
   },
   "id": "18f1bb4a3fc83cd6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Train data loading\n",
    "try:\n",
    "    train_df = pd.read_pickle(PICKLES_FOLDER + 'train_df.pkl')\n",
    "except:\n",
    "    train_df = pd.read_csv(DATA_FOLDER + 'train.csv')\n",
    "    train_df.to_pickle(PICKLES_FOLDER + 'train_df.pkl')\n",
    "\n",
    "# Test data loading  \n",
    "try:\n",
    "    test_df = pd.read_pickle(PICKLES_FOLDER + 'test_df.pkl')\n",
    "except:\n",
    "    test_df = pd.read_csv(DATA_FOLDER + 'test.csv')\n",
    "    test_df.to_pickle(PICKLES_FOLDER + 'test_df.pkl')\n",
    "\n",
    "# Redirects loading\n",
    "try:\n",
    "    enwinki_redirects = pd.read_pickle(PICKLES_FOLDER + 'enwiki_redirects.pkl')\n",
    "except:\n",
    "    enwinki_redirects = pd.read_csv(WIKILITE_FOLDER + 'enwiki_redirects.tsv', names=['en_title', 'en_redirect_title'],\n",
    "                                    sep='\\t')\n",
    "    enwinki_redirects.to_pickle(PICKLES_FOLDER + 'enwiki_redirects.pkl')\n",
    "\n",
    "# Aliases loading  \n",
    "try:\n",
    "    item_aliases = pd.read_pickle(PICKLES_FOLDER + 'item_aliases.pkl')\n",
    "except:\n",
    "    item_aliases = pd.read_csv(WIKILITE_FOLDER + 'item_aliases.csv')\n",
    "    item_aliases.to_pickle(PICKLES_FOLDER + 'item_aliases.pkl')\n",
    "\n",
    "# Properties loading\n",
    "try:\n",
    "    properties = pd.read_pickle(PICKLES_FOLDER + 'property.pkl')\n",
    "except:\n",
    "    properties = pd.read_csv(WIKILITE_FOLDER + 'property.csv')\n",
    "    properties.to_pickle(PICKLES_FOLDER + 'property.pkl')\n",
    "\n",
    "# Statements loading\n",
    "try:\n",
    "    statements = pd.read_pickle(PICKLES_FOLDER + 'statements.pkl')\n",
    "except:\n",
    "    statements = pd.read_csv(WIKILITE_FOLDER + 'statements.csv')\n",
    "    statements.to_pickle(PICKLES_FOLDER + 'statements.pkl')\n",
    "\n",
    "# Wiki items loading \n",
    "try:\n",
    "    wiki_items = pd.read_pickle(PICKLES_FOLDER + 'wiki_items.pkl')\n",
    "except:\n",
    "    wiki_items = pd.read_csv(WIKILITE_FOLDER + 'wiki_items.csv')\n",
    "    wiki_items.to_pickle(PICKLES_FOLDER + 'wiki_items.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:33.534265100Z",
     "start_time": "2023-12-17T15:34:29.643381200Z"
    }
   },
   "id": "8b9e6f0ee2813fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Using existing datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74aa5a0e772b9c9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f73f63d272f417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merging `item_aliases` and `wiki_items` on `item_id` to get the `wikipedia_title` for each `en_alias`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b64eefea3ce0942"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   item_id  en_label                                 en_description  \\\n0        1  Universe             totality of space and all contents   \n1        1  Universe             totality of space and all contents   \n2        1  Universe             totality of space and all contents   \n3        1  Universe             totality of space and all contents   \n4        2     Earth  third planet from the Sun in the Solar System   \n\n  wikipedia_title      en_alias  \n0        Universe  Our Universe  \n1        Universe  The Universe  \n2        Universe    The Cosmos  \n3        Universe        cosmos  \n4           Earth   Blue Planet  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_id</th>\n      <th>en_label</th>\n      <th>en_description</th>\n      <th>wikipedia_title</th>\n      <th>en_alias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>Our Universe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>The Universe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>The Cosmos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>cosmos</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Earth</td>\n      <td>third planet from the Sun in the Solar System</td>\n      <td>Earth</td>\n      <td>Blue Planet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_wiki_items = wiki_items.merge(item_aliases, how='left', on='item_id')\n",
    "merged_wiki_items.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:35.067064200Z",
     "start_time": "2023-12-17T15:34:33.538273900Z"
    }
   },
   "id": "bd280c1982f55afb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Copying the dataframes to modify them\n",
    "test_df_mod = test_df.copy(deep=True)\n",
    "train_df_mod = train_df.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:35.078091500Z",
     "start_time": "2023-12-17T15:34:35.067064200Z"
    }
   },
   "id": "82965c83ebbb3015"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Beginning of the URL to wikipedia\n",
    "URL = 'http://en.wikipedia.org/wiki/'\n",
    "LEN_URL = len(URL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:35.090337200Z",
     "start_time": "2023-12-17T15:34:35.080262800Z"
    }
   },
   "id": "2cb2450d10518b95"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# We need to lowercase the tokens to avoid problems with case in future steps\n",
    "train_df_mod['full_mention_lower'] = train_df_mod['full_mention'].str.lower()\n",
    "\n",
    "# We only keep the tokens that have a wiki_url\n",
    "train_df_mod = train_df_mod[train_df_mod['wiki_url'].notnull() & (train_df_mod['wiki_url'] != '--NME--')]\n",
    "\n",
    "# We also lowercase the tokens in the test data for future steps\n",
    "test_df_mod['full_mention'] = test_df_mod['full_mention'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:35.176793Z",
     "start_time": "2023-12-17T15:34:35.083785700Z"
    }
   },
   "id": "7249692acea2e0eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform:\n",
    "- `en_alias` and `wikipedia_title` in `merged_wiki_items`\n",
    "- `wikipedia_title` in `wiki_items`\n",
    "- `en_title` in `redirects`\n",
    "\n",
    "to lowercase to avoid problems with case in future steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd5f505f0444795b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "merged_wiki_items['en_alias_lower'] = merged_wiki_items['en_alias'].str.lower()\n",
    "merged_wiki_items['wikipedia_title_lower'] = merged_wiki_items['wikipedia_title'].str.lower()\n",
    "\n",
    "wiki_items['wikipedia_title_lower'] = wiki_items['wikipedia_title'].str.lower()\n",
    "\n",
    "enwinki_redirects['en_title_lower'] = enwinki_redirects['en_title'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:38.694281800Z",
     "start_time": "2023-12-17T15:34:35.121925300Z"
    }
   },
   "id": "7d96a359b740d910"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating dictionaries for faster lookup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef8e3b286066879e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "DICT_FOLDER = PICKLES_FOLDER + 'dictionaries/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:38.703338400Z",
     "start_time": "2023-12-17T15:34:38.695292100Z"
    }
   },
   "id": "cf3e7bcd5309c070"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "try:\n",
    "    aliases_dict = pickle.load(open(DICT_FOLDER + 'aliases_dict.pkl', 'rb'))\n",
    "except:\n",
    "    aliases_dict = pd.Series(merged_wiki_items['wikipedia_title'].values,\n",
    "                             index=merged_wiki_items['en_alias_lower']).to_dict()\n",
    "    pickle.dump(aliases_dict, open(DICT_FOLDER + 'aliases_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:39.303936Z",
     "start_time": "2023-12-17T15:34:38.703338400Z"
    }
   },
   "id": "82d4af9957872fe7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "try:\n",
    "    titles_dict = pickle.load(open(DICT_FOLDER + 'titles_dict.pkl', 'rb'))\n",
    "except:\n",
    "    titles_dict = pd.Series(wiki_items['wikipedia_title'].values, index=wiki_items['wikipedia_title_lower']).to_dict()\n",
    "    pickle.dump(titles_dict, open(DICT_FOLDER + 'titles_dict.pkl', 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:41.812587500Z",
     "start_time": "2023-12-17T15:34:39.305944300Z"
    }
   },
   "id": "e2a837f701077025"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dict = pickle.load(open(DICT_FOLDER + 'train_dict.pkl', 'rb'))\n",
    "except:\n",
    "    train_dict = pd.Series(train_df_mod['wiki_url'].values, index=train_df_mod['full_mention_lower']).to_dict()\n",
    "    pickle.dump(train_dict, open(DICT_FOLDER + 'train_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:41.828625Z",
     "start_time": "2023-12-17T15:34:41.815708100Z"
    }
   },
   "id": "d6c486944ee1036d"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "try:\n",
    "    redirects_dict = pickle.load(open(DICT_FOLDER + 'redirects_dict.pkl', 'rb'))\n",
    "except:\n",
    "    redirects_dict = pd.Series(enwinki_redirects['en_redirect_title'].values,\n",
    "                               index=enwinki_redirects['en_title_lower']).to_dict()\n",
    "    pickle.dump(redirects_dict, open(DICT_FOLDER + 'redirects_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:46.143941700Z",
     "start_time": "2023-12-17T15:34:41.830634800Z"
    }
   },
   "id": "252ec3cd41695de4"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/104890 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8941210719904a7b8a0d49dda89eefd5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We want to keep the count of the number of matches we find with aliases and redirects\n",
    "aliases_matching = 0\n",
    "redirects_matching = 0\n",
    "\n",
    "for index, row in tqdm(test_df_mod.iterrows(), total=test_df_mod.shape[0]):\n",
    "    if str(row['wiki_url']) == 'nan' or row['wiki_url'] != '?':\n",
    "        continue\n",
    "\n",
    "    token = row['full_mention']\n",
    "    train_url = train_dict.get(token)\n",
    "\n",
    "    if train_url is not None:\n",
    "        # We found a link in the train data and it is the true identity so we can use it\n",
    "        test_df_mod.at[index, 'wiki_url'] = train_url\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        # TODO: explain the order of the if statements in the report\n",
    "        wiki_title = titles_dict.get(token)\n",
    "\n",
    "        if wiki_title is not None:\n",
    "            aliases_matching += 1\n",
    "\n",
    "        else:\n",
    "            wiki_title = aliases_dict.get(token)\n",
    "\n",
    "            if wiki_title is not None:\n",
    "                aliases_matching += 1\n",
    "\n",
    "    if wiki_title is not None:\n",
    "        redirect_title = redirects_dict.get(wiki_title.lower())\n",
    "\n",
    "        if redirect_title is not None:\n",
    "            redirects_matching += 1\n",
    "            test_df_mod.at[index, 'wiki_url'] = URL + redirect_title.replace(' ', '_')\n",
    "\n",
    "        else:\n",
    "            test_df_mod.at[index, 'wiki_url'] = URL + wiki_title.replace(' ', '_')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:48.931742900Z",
     "start_time": "2023-12-17T15:34:46.143941700Z"
    }
   },
   "id": "8e186a6eee7eb6ba"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "2107"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aliases_matching"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:48.938461700Z",
     "start_time": "2023-12-17T15:34:48.931742900Z"
    }
   },
   "id": "416956ed3cbe7456"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "393"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirects_matching"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:48.993352100Z",
     "start_time": "2023-12-17T15:34:48.936900300Z"
    }
   },
   "id": "38d44acc9170b212"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how many tokens we found links for:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7507b3a85b5a8586"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously we had 9166 tokens without a link\n",
      "Now we have 547 tokens without a link\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previously we had {test_df[test_df['wiki_url'] == '?']['wiki_url'].count()} tokens without a link\")\n",
    "print(f\"Now we have {test_df_mod[test_df_mod['wiki_url'] == '?']['wiki_url'].count()} tokens without a link\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:48.994874600Z",
     "start_time": "2023-12-17T15:34:48.954164800Z"
    }
   },
   "id": "c3e735d9fe28d308"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating submission file for second part"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2f5746f1b4d3123"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "result_first_part = test_df_mod[['id', 'wiki_url']]\n",
    "result_first_part.loc[:, 'wiki_url'] = result_first_part['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http') or str(x) == '?') else x)\n",
    "# result_first_part.loc[:, 'wiki_url'] = result_first_part['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http')) else x)\n",
    "\n",
    "# name = 'tr_title_alias_dict_redirects_url'\n",
    "# \n",
    "# result_first_part.to_csv(SUBMISSIONS_FOLDER + name + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:49.022529800Z",
     "start_time": "2023-12-17T15:34:48.974314200Z"
    }
   },
   "id": "df485ff6287f478b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f374ebae8b86d158"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def get_connection(only_train=False):\n",
    "    \"\"\"\n",
    "    Function retrieving the connection between the wiki_items from the statements.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    only_train: bool, optional (default=False)\n",
    "        If True, only the connections between the items in the train data are returned.\n",
    "    \n",
    "    Returns\n",
    "    -------  \n",
    "    connection: pd.DataFrame\n",
    "        DataFrame containing the connections between the wiki_items.\n",
    "    \"\"\"\n",
    "    \n",
    "    connection = statements\n",
    "    if only_train:\n",
    "        list_of_ids = pd.read_csv('train_wiki.csv')  # obtain by running get_wiki.ipynb\n",
    "        list_of_ids = list_of_ids.item_id.tolist()\n",
    "        all_ids = set(connection['source_item_id'].unique()).union(set(connection['target_item_id'].unique()))\n",
    "\n",
    "        filtered_ids = set(list_of_ids).intersection(all_ids)\n",
    "\n",
    "        connection = connection[\n",
    "            connection['source_item_id'].isin(filtered_ids) | connection['target_item_id'].isin(filtered_ids)]\n",
    "    return connection\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:49.022529800Z",
     "start_time": "2023-12-17T15:34:49.010753600Z"
    }
   },
   "id": "f2d633317a1327cd"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def add_edges(G, connection, progress=True):\n",
    "    \"\"\"\n",
    "    Function adding the edges to the graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G: nx.Graph\n",
    "        Graph to which the edges are added.\n",
    "    connection: pd.DataFrame\n",
    "        DataFrame containing the connections between the wiki_items.\n",
    "    progress: bool, optional (default=True)\n",
    "        If True, a progress bar is shown.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    G: nx.Graph\n",
    "        Graph with the added edges. \n",
    "    \"\"\"\n",
    "    \n",
    "    if progress:\n",
    "        for source_item_id, _, target_item_id in tqdm(connection.iloc, total=len(connection)):\n",
    "            G.add_edge(source_item_id, target_item_id)\n",
    "    else:\n",
    "        G = nx.from_pandas_edgelist(connection, 'source_item_id', 'target_item_id', create_using=nx.Graph)\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:49.086492200Z",
     "start_time": "2023-12-17T15:34:49.012270600Z"
    }
   },
   "id": "e80f9d6941dae5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "PICKLE_FILENAME = \"graph_undirected_full.pkl\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:34:49.090011800Z",
     "start_time": "2023-12-17T15:34:49.022529800Z"
    }
   },
   "id": "4e5f0950aac4c8de"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the graph from the pickle file\n",
    "    with open(PICKLES_FOLDER + PICKLE_FILENAME, 'rb') as pickle_file:\n",
    "        G = pickle.load(pickle_file)\n",
    "except:\n",
    "    connection = get_connection()\n",
    "    G = nx.Graph()  # Undirected Graph\n",
    "    G = add_edges(G, connection, progress=True)\n",
    "    \n",
    "    # Save graph pickle to the file\n",
    "    with open(PICKLES_FOLDER + PICKLE_FILENAME, 'wb') as pickle_file:\n",
    "        pickle.dump(G, pickle_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:20.119150400Z",
     "start_time": "2023-12-17T15:34:49.030790100Z"
    }
   },
   "id": "6d20743712452f8e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4906271\n",
      "Number of edges: 24528246\n"
     ]
    }
   ],
   "source": [
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:21.637958400Z",
     "start_time": "2023-12-17T15:35:20.120150700Z"
    }
   },
   "id": "789a31308f2cfcff"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN             NaN   \n1            1                        CRICKET        NaN             NaN   \n2            2                              -        NaN             NaN   \n3            3                 LEICESTERSHIRE          B  LEICESTERSHIRE   \n4            4                           TAKE        NaN             NaN   \n...        ...                            ...        ...             ...   \n104885  104885                        brother        NaN             NaN   \n104886  104886                              ,        NaN             NaN   \n104887  104887                          Bobby          B           Bobby   \n104888  104888                              .        NaN             NaN   \n104889  104889                            NaN        NaN             NaN   \n\n       wiki_url_x                                         wiki_url_y  \n0             NaN                                          NOT_FOUND  \n1             NaN                                          NOT_FOUND  \n2             NaN                                          NOT_FOUND  \n3               ?  http://en.wikipedia.org/wiki/Leicestershire_Co...  \n4             NaN                                          NOT_FOUND  \n...           ...                                                ...  \n104885        NaN                                          NOT_FOUND  \n104886        NaN                                          NOT_FOUND  \n104887          ?                 http://en.wikipedia.org/wiki/Bobby  \n104888        NaN                                          NOT_FOUND  \n104889        NaN                                          NOT_FOUND  \n\n[104890 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>full_mention</th>\n      <th>wiki_url_x</th>\n      <th>wiki_url_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>LEICESTERSHIRE</td>\n      <td>?</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>Bobby</td>\n      <td>?</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking the result from the first part\n",
    "partial = result_first_part\n",
    "\n",
    "testdf = test_df.copy(deep=True)\n",
    "testdf = testdf.merge(partial, on='id')\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:21.695317200Z",
     "start_time": "2023-12-17T15:35:21.635957600Z"
    }
   },
   "id": "c74cc83c41c2746b"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "         id                    en_redirect_title\n0       795   Universities in the United Kingdom\n1       975   Universities in the United Kingdom\n2      4396                           David Barr\n3      4402                     Michael Sullivan\n4      4552  United States Amateur Championships\n..      ...                                  ...\n342  103998                    Predrag Mijatović\n343  104041                    Predrag Mijatović\n344  104124                            Mijatović\n345  104310                        De Graafschap\n346  104877                  1966 FIFA World Cup\n\n[347 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>en_redirect_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>795</td>\n      <td>Universities in the United Kingdom</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>975</td>\n      <td>Universities in the United Kingdom</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4396</td>\n      <td>David Barr</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4402</td>\n      <td>Michael Sullivan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4552</td>\n      <td>United States Amateur Championships</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>103998</td>\n      <td>Predrag Mijatović</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>104041</td>\n      <td>Predrag Mijatović</td>\n    </tr>\n    <tr>\n      <th>344</th>\n      <td>104124</td>\n      <td>Mijatović</td>\n    </tr>\n    <tr>\n      <th>345</th>\n      <td>104310</td>\n      <td>De Graafschap</td>\n    </tr>\n    <tr>\n      <th>346</th>\n      <td>104877</td>\n      <td>1966 FIFA World Cup</td>\n    </tr>\n  </tbody>\n</table>\n<p>347 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "redirect = enwinki_redirects\n",
    "temp = testdf[partial.wiki_url == \"?\"].merge(redirect, left_on='full_mention', right_on='en_title')[['id', 'en_redirect_title']]\n",
    "\n",
    "display(temp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:27.418029100Z",
     "start_time": "2023-12-17T15:35:21.670244200Z"
    }
   },
   "id": "6070da4a30bc3db4"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN             NaN   \n1            1                        CRICKET        NaN             NaN   \n2            2                              -        NaN             NaN   \n3            3                 LEICESTERSHIRE          B  LEICESTERSHIRE   \n4            4                           TAKE        NaN             NaN   \n...        ...                            ...        ...             ...   \n104885  104885                        brother        NaN             NaN   \n104886  104886                              ,        NaN             NaN   \n104887  104887                          Bobby          B           Bobby   \n104888  104888                              .        NaN             NaN   \n104889  104889                            NaN        NaN             NaN   \n\n                                                 wiki_url en_redirect_title  \n0                                               NOT_FOUND               NaN  \n1                                               NOT_FOUND               NaN  \n2                                               NOT_FOUND               NaN  \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...    LEICESTERSHIRE  \n4                                               NOT_FOUND               NaN  \n...                                                   ...               ...  \n104885                                          NOT_FOUND               NaN  \n104886                                          NOT_FOUND               NaN  \n104887                 http://en.wikipedia.org/wiki/Bobby             Bobby  \n104888                                          NOT_FOUND               NaN  \n104889                                          NOT_FOUND               NaN  \n\n[104890 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n      <th>en_redirect_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>LEICESTERSHIRE</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n      <td>LEICESTERSHIRE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>Bobby</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n      <td>Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdf = testdf.merge(temp, on='id', how='left')\n",
    "testdf['en_redirect_title'] = testdf.apply(lambda row: row.full_mention if pd.isna(row['en_redirect_title']) else row['en_redirect_title'], axis=1)\n",
    "\n",
    "testdf = testdf.drop(columns=['wiki_url_x'])\n",
    "testdf.rename(columns={'wiki_url_y': 'wiki_url'}, inplace=True)\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:28.356781700Z",
     "start_time": "2023-12-17T15:35:27.419028400Z"
    }
   },
   "id": "4cd6d5c1e8498977"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "          item_id                 en_label          wikipedia_title\n0               1                 Universe                 Universe\n1               2                    Earth                    Earth\n2               3                     life                     Life\n3               4                    death                    Death\n4               5                    human                    Human\n...           ...                      ...                      ...\n5216231  77042017                  HR 4523                HD 102365\n5216232  77043280         Charlie Johnston        Charlie Johnstone\n5216233  77231860               Aldo Rossi    Aldo Rossi (musician)\n5216234  77240068  Ebenezer Baptist Church  Ebenezer Baptist Church\n5216235  77242291                New Court                New Court\n\n[5216236 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_id</th>\n      <th>en_label</th>\n      <th>wikipedia_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>Universe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Earth</td>\n      <td>Earth</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>life</td>\n      <td>Life</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>death</td>\n      <td>Death</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>human</td>\n      <td>Human</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5216231</th>\n      <td>77042017</td>\n      <td>HR 4523</td>\n      <td>HD 102365</td>\n    </tr>\n    <tr>\n      <th>5216232</th>\n      <td>77043280</td>\n      <td>Charlie Johnston</td>\n      <td>Charlie Johnstone</td>\n    </tr>\n    <tr>\n      <th>5216233</th>\n      <td>77231860</td>\n      <td>Aldo Rossi</td>\n      <td>Aldo Rossi (musician)</td>\n    </tr>\n    <tr>\n      <th>5216234</th>\n      <td>77240068</td>\n      <td>Ebenezer Baptist Church</td>\n      <td>Ebenezer Baptist Church</td>\n    </tr>\n    <tr>\n      <th>5216235</th>\n      <td>77242291</td>\n      <td>New Court</td>\n      <td>New Court</td>\n    </tr>\n  </tbody>\n</table>\n<p>5216236 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_item = wiki_items[['item_id', 'en_label', 'wikipedia_title']]\n",
    "# wiki_item = wiki_item[wiki_item['item_id'].isin(filtered_ids)]\n",
    "\n",
    "# wiki_item['en_label'] = wiki_item['en_label'] + \" \" + wiki_item['wikipedia_title']\n",
    "col_ = wiki_item.wikipedia_title.str.lower().str\n",
    "lower_case_wiki_item_titles = wiki_item.wikipedia_title.str.lower().str\n",
    "display(wiki_item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:30.235954500Z",
     "start_time": "2023-12-17T15:35:28.391904100Z"
    }
   },
   "id": "b7c7606df31d5ffe"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN             NaN   \n1            1                        CRICKET        NaN             NaN   \n2            2                              -        NaN             NaN   \n3            3                 LEICESTERSHIRE          B  LEICESTERSHIRE   \n4            4                           TAKE        NaN             NaN   \n...        ...                            ...        ...             ...   \n104885  104885                        brother        NaN             NaN   \n104886  104886                              ,        NaN             NaN   \n104887  104887                          Bobby          B           Bobby   \n104888  104888                              .        NaN             NaN   \n104889  104889                            NaN        NaN             NaN   \n\n                                                 wiki_url en_redirect_title  \\\n0                                               NOT_FOUND               NaN   \n1                                               NOT_FOUND               NaN   \n2                                               NOT_FOUND               NaN   \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...    LEICESTERSHIRE   \n4                                               NOT_FOUND               NaN   \n...                                                   ...               ...   \n104885                                          NOT_FOUND               NaN   \n104886                                          NOT_FOUND               NaN   \n104887                 http://en.wikipedia.org/wiki/Bobby             Bobby   \n104888                                          NOT_FOUND               NaN   \n104889                                          NOT_FOUND               NaN   \n\n          item_id  \n0             NaN  \n1             NaN  \n2             NaN  \n3       3229147.0  \n4             NaN  \n...           ...  \n104885        NaN  \n104886        NaN  \n104887   289262.0  \n104888        NaN  \n104889        NaN  \n\n[104890 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n      <th>en_redirect_title</th>\n      <th>item_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>LEICESTERSHIRE</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n      <td>LEICESTERSHIRE</td>\n      <td>3229147.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>Bobby</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n      <td>Bobby</td>\n      <td>289262.0</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testdf['wiki_title'] = testdf.wiki_url.str[LEN_URL:].str.replace('_', ' ')\n",
    "testdf = testdf.merge(wiki_item, left_on='wiki_title', right_on='wikipedia_title', how='left').drop(\n",
    "    columns=['wikipedia_title', 'en_label', 'wiki_title'])\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.084988900Z",
     "start_time": "2023-12-17T15:35:30.241696Z"
    }
   },
   "id": "caf97f2b1b17be22"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def get_dist(certain_list, candidate, fill_na=9999):\n",
    "    \"\"\"\n",
    "    Function calculating the distance between the full mention and a candidate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    certain_list: list\n",
    "        List containing the already assigned entities.\n",
    "    candidate: float\n",
    "        Candidate id.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distance: float\n",
    "        Distance between the full mention and the candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance_list = []\n",
    "    subset_certain = list(certain_list)\n",
    "    random.shuffle(subset_certain)\n",
    "    subset_size = 10\n",
    "    subset_certain = subset_certain[:subset_size]\n",
    "    \n",
    "    for assigned_entity in subset_certain:\n",
    "        try:\n",
    "            shortest = nx.shortest_path_length(G, source=assigned_entity, target=candidate)\n",
    "            distance_list.append(shortest)\n",
    "        except:\n",
    "            distance_list.append(fill_na)\n",
    "            \n",
    "    return sum(distance_list) / subset_size if distance_list else fill_na"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.093900800Z",
     "start_time": "2023-12-17T15:35:33.088636500Z"
    }
   },
   "id": "a769a09c2239499"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def get_all_dist(filtered_ls, full_mention_train):\n",
    "    \"\"\"\n",
    "    Function calculating the distance between the full mention and all the candidates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filtered_ls: pd.DataFrame\n",
    "        DataFrame containing the candidates.\n",
    "    full_mention_train: list\n",
    "        List of the already assigned entities.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distances: list\n",
    "        List containing the distances between the full mention and all the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for candidate, _, title in filtered_ls.iloc:\n",
    "        distances.append((title, candidate, get_dist(full_mention_train, candidate)))\n",
    "        \n",
    "    return distances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.128605Z",
     "start_time": "2023-12-17T15:35:33.093296600Z"
    }
   },
   "id": "b3d5f1e3fa796f46"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def get_info(current_document):\n",
    "    \"\"\"\n",
    "    Function retrieving the information from the current document.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_document: list\n",
    "        List containing all the rows of the current document.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    found: pd.DataFrame\n",
    "        DataFrame containing the rows of the current document that have a link.\n",
    "    not_found: pd.DataFrame\n",
    "        DataFrame containing the rows of the current document that have to be assigned.\n",
    "    item_id_train: set\n",
    "        Set containing the ids of the entities that have a link.\n",
    "    full_mention_found: dict\n",
    "        Dictionary containing the full mentions that have a link and the corresponding link.\n",
    "    mention_test: set\n",
    "        Set containing the full mentions that have to be assigned.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(current_document):\n",
    "        document_df = pd.DataFrame(current_document)\n",
    "        full_mention_ = document_df[document_df['wiki_url'] != 'NOT_FOUND']\n",
    "        found = full_mention_[full_mention_['wiki_url'] != '?']\n",
    "        item_id_train = set(found.item_id.tolist())\n",
    "        full_mention_found = dict(zip(found['full_mention'].str.lower(), found['wiki_url']))\n",
    "\n",
    "        not_found = full_mention_[full_mention_['wiki_url'] == '?'].copy()\n",
    "        not_found.full_mention = not_found.full_mention.str.lower()\n",
    "        mention_test = set(not_found.full_mention.tolist())\n",
    "        \n",
    "        return found, not_found, item_id_train, full_mention_found, mention_test\n",
    "    \n",
    "    else:\n",
    "        return None, None, None, None, None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.160688700Z",
     "start_time": "2023-12-17T15:35:33.110333900Z"
    }
   },
   "id": "8cf28f991162537d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def find_doc_range(df):\n",
    "    \"\"\"\n",
    "    Function returning a zipped tuple of end indexes and start indexes of documents.\n",
    "    \"\"\"\n",
    "    # TODO doc\n",
    "    def check_docstart(row):\n",
    "        \"\"\"\n",
    "        True if the current row is the row where the document starts.\n",
    "        \"\"\"\n",
    "        if pd.notnull(row['token']) and 'DOCSTART' in row['token']:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    data = df.copy()\n",
    "    data['docstart_id'] = df.apply(check_docstart, axis=1)\n",
    "    start_ids = data[data['docstart_id']]['id'].values\n",
    "    print(len(start_ids))\n",
    "    end_ids = start_ids[1:] - 1\n",
    "    end_ids = np.append(end_ids,len(df))\n",
    "    docs_range = zip(start_ids, end_ids)\n",
    "    \n",
    "    return docs_range"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.181278900Z",
     "start_time": "2023-12-17T15:35:33.118968900Z"
    }
   },
   "id": "67fa300bff9ad2e3"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def split_document_findings(document_dataframe):\n",
    "    \"\"\"\n",
    "    Function retrieving and separating entities depending on weather they are associated with a found url or not.\n",
    "    \"\"\"\n",
    "    #TODO doc.\n",
    "    document = document_dataframe[document_dataframe['wiki_url'] != 'NOT_FOUND']\n",
    "    found_links = document[document['wiki_url'] != '?']\n",
    "    not_found_links = document[document['wiki_url'] == '?']\n",
    "    return document, found_links, not_found_links"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.182278600Z",
     "start_time": "2023-12-17T15:35:33.124091600Z"
    }
   },
   "id": "39c20eb8f5859293"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def find_equal_string_candidates(mention, full_mention_found):\n",
    "    #TODO doc.\n",
    "    return list(set([url for found_mention, url in full_mention_found.items() if mention in found_mention]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:41:10.233363200Z",
     "start_time": "2023-12-17T15:41:10.227249Z"
    }
   },
   "id": "42657415b7e35c00"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def find_substring_candidates(mention, saved_candidates):\n",
    "    #TODO make sure all dataframe have a lower case version for comparaison and use it.\n",
    "    #TODO doc.\n",
    "    if mention not in saved_candidates.keys(): \n",
    "        candidates = wiki_item.loc[lower_case_wiki_item_titles.contains(r'\\b{}\\b'.format(mention), na=False)]\n",
    "        saved_candidates[mention] = candidates\n",
    "        return candidates\n",
    "    else:\n",
    "        return saved_candidates[uncertain_word]\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.228205Z",
     "start_time": "2023-12-17T15:35:33.139655100Z"
    }
   },
   "id": "5a7fb74273865f75"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def find_best_candidate(candidates_for_mention, found_ids):\n",
    "    \"\"\"\n",
    "    Function returning the best candidates for the mention.\n",
    "    \"\"\"\n",
    "    distances = get_all_dist(candidates_for_mention, found_ids)\n",
    "    obtained_distances = len(distances)\n",
    "    if obtained_distances == 1:\n",
    "        title, _, _ = distances[0]\n",
    "        return URL + title.replace(' ', '_')\n",
    "    elif obtained_distances >=1:\n",
    "        first_candidate, second_candidate = heapq.nsmallest(2, distances,key=lambda x: x[-1])       \n",
    "        if first_candidate[-1] < second_candidate[-1]:\n",
    "            title, _, _ = first_candidate\n",
    "            return URL + title.replace(' ', '_')\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.246187800Z",
     "start_time": "2023-12-17T15:35:33.147169300Z"
    }
   },
   "id": "f32f033ad499a985"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the global constants and variables used for graph decisions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d866ca71704049"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "MAX_CANDIDATES = 50\n",
    "MAX_FETCHING_TRIES = 3\n",
    "SEED = 42\n",
    "OLD_SIZE = np.inf\n",
    "current_document = []\n",
    "saved_candidates = {}\n",
    "\n",
    "random.seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:35:33.247188700Z",
     "start_time": "2023-12-17T15:35:33.153630700Z"
    }
   },
   "id": "7036f038c34356b6"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    }
   ],
   "source": [
    "testdf['full_mention'] = testdf['full_mention'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:38:40.127278100Z",
     "start_time": "2023-12-17T15:38:39.659717800Z"
    }
   },
   "id": "5859e944deadf52"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "104343"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testdf[~(testdf['wiki_url'] == '?')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:38:42.203866500Z",
     "start_time": "2023-12-17T15:38:42.190511900Z"
    }
   },
   "id": "20ca1ab11b3d1a72"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 31\u001B[0m\n\u001B[0;32m     28\u001B[0m single_candidates \u001B[38;5;241m=\u001B[39m candidates_urls[len_candidates_urls \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, row \u001B[38;5;129;01min\u001B[39;00m single_candidates\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m#TODO check indexes not messed up\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m     found_links\u001B[38;5;241m.\u001B[39mat[row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwiki_url\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcandidates_url\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Update not found links\u001B[39;00m\n\u001B[0;32m     34\u001B[0m not_found_links \u001B[38;5;241m=\u001B[39m not_found_links[\u001B[38;5;241m~\u001B[39mnot_found_links[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39misin(single_candidates[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_id\u001B[39m\u001B[38;5;124m'\u001B[39m])]\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1095\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1053\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         main_debugger\u001B[38;5;241m.\u001B[39mdo_wait_suspend(thread, frame, event, arg)\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "docs_range = find_doc_range(testdf)\n",
    "\n",
    "for start_ids, end_ids in docs_range:\n",
    "    current_document = testdf.iloc[start_ids:end_ids]\n",
    "    # print(current_document)\n",
    "    \n",
    "    #TODO make sure that everything is lowercase when comparing strings. IN ALL FUNCTIONS.\n",
    "    \n",
    "    # We are interested in the rows that should have a link\n",
    "    current_document, found_links, not_found_links = split_document_findings(current_document)\n",
    "    if len(not_found_links) == 0:\n",
    "        continue\n",
    "    \n",
    "    found_links_count = len(found_links)\n",
    "    \n",
    "    for _ in range(MAX_FETCHING_TRIES):\n",
    "        #Shuffle mentions TODO check shuffle works and does not mess up the indexes\n",
    "        random_mentions = not_found_links[['item_id','full_mention']].sample(frac=1)\n",
    "        \n",
    "        #Find candidates\n",
    "        candidates_urls = pd.DataFrame({\n",
    "            'item_id': random_mentions['item_id'],\n",
    "            'candidates_url': random_mentions['full_mention'].apply(lambda mention: find_equal_string_candidates(mention, dict(zip(found_links['full_mention'], found_links['wiki_url']))))\n",
    "        })\n",
    "        len_candidates_urls = candidates_urls['candidates_url'].apply(len)\n",
    "        \n",
    "        #for single candidates, retrieve the value and attribute it.\n",
    "        single_candidates = candidates_urls[len_candidates_urls == 1]\n",
    "        for idx, row in single_candidates.iterrows():\n",
    "            #TODO check indexes not messed up\n",
    "            found_links.at[row['item_id'], 'wiki_url'] = row['candidates_url'][0]\n",
    "        \n",
    "        # Update not found links\n",
    "        not_found_links = not_found_links[~not_found_links['item_id'].isin(single_candidates['item_id'])]\n",
    "\n",
    "        \n",
    "        #try finding the best for substrings\n",
    "        candidates_urls = pd.DataFrame({\n",
    "            'item_id': random_mentions['item_id'],\n",
    "            'candidates_url': random_mentions['full_mention'].apply(lambda mention: find_substring_candidates(mention, saved_candidates))\n",
    "        })\n",
    "        \n",
    "        # Calculate the number of candidates for each mention\n",
    "        len_candidates_urls = candidates_urls['candidates_url'].apply(len)\n",
    "        \n",
    "        # Process single candidates\n",
    "        single_candidates = candidates_urls[len_candidates_urls == 1]\n",
    "        for idx, row in single_candidates.iterrows():\n",
    "            #TODO check indexes not messed up\n",
    "            found_links.at[idx, 'wiki_url'] = row['candidates_url'][0]\n",
    "        \n",
    "        # Filter for multi candidates\n",
    "        multi_candidates = candidates_urls[(len_candidates_urls > 1) & (len_candidates_urls <= MAX_CANDIDATES)]\n",
    "        \n",
    "        # Apply find_best_candidate for each mention in multi_candidates\n",
    "        for not_found_id in multi_candidates['item_id']:\n",
    "            #TODO check indexes not messed up\n",
    "            best_candidate = find_best_candidate(multi_candidates[multi_candidates['item_id'] == not_found_id]['candidates_url'].iloc[0], found_links['item_id'])\n",
    "            \n",
    "            if best_candidate is not None:\n",
    "                found_links.at['item_id', 'wiki_url'] = best_candidate\n",
    "        \n",
    "        if found_links_count < len(found_links):\n",
    "            found_links_count = len(found_links)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "# TODO re-concat not_found and found."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T16:11:37.102808800Z",
     "start_time": "2023-12-17T15:51:43.679245Z"
    }
   },
   "id": "d7d41bff18135424"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "Processing:   0%|          | 0/104890 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "979be66114f4473c8b3d025aff630a1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essex has too many candidates 618\n",
      "can not decide between ('Tom Moody', 3537597, 9999) and ('Tom Moody (artist)', 7816917, 9999)\n",
      "simmons has too many candidates 446\n",
      "can not decide between ('Phil Simmons', 6827914, 9999) and ('Phil Simmons (rower)', 47434073, 9999)\n",
      "nottinghamshire has too many candidates 215\n",
      "oval has too many candidates 211\n",
      "hussein has too many candidates 384\n",
      "derbyshire has too many candidates 435\n",
      "can not decide between ('Headingley Stadium', 1187032, 0.2) and ('List of international cricket centuries at Headingley', 6624161, 0.2)\n",
      "leicestershire has too many candidates 167\n",
      "surrey has too many candidates 387\n",
      "yorkshire has too many candidates 1055\n",
      "london has too many candidates 4659\n",
      "can not decide between ('Chris Lewis (tennis)', 1077498, 0.4) and ('Chris Lewis-Harris', 5107231, 0.4)\n",
      "worcestershire has too many candidates 180\n",
      "somerset has too many candidates 693\n",
      "such has too many candidates 100\n",
      "can not decide between ('Nasser Hussain (rugby union)', 3336238, 0.4) and ('Nasser Hussain', 3520174, 0.4)\n",
      "kent has too many candidates 1556\n",
      "england has too many candidates 2693\n",
      "australians has too many candidates 164\n",
      "can not decide between ('Paul Johnson (writer)', 45997, 0.8) and ('Paul Johnson (squash player)', 2278396, 0.8)\n",
      "west indian has too many candidates 198\n",
      "warwickshire has too many candidates 197\n",
      "can not decide between ('Chris Adams (wrestler)', 860227, 1.0) and ('Chris Adams (footballer)', 5105681, 1.0)\n",
      "can not decide between ('Tom Moody', 3537597, 1.3) and ('Tom Moody (artist)', 7816917, 1.3)\n",
      "can not decide between ('Phil Simmons', 6827914, 1.2) and ('Phil Simmons (rower)', 47434073, 1.2)\n",
      "simmons has too many candidates 446\n",
      "hussein has too many candidates 384\n",
      "london has too many candidates 4659\n",
      "kent has too many candidates 1556\n",
      "nottinghamshire has too many candidates 215\n",
      "can not decide between ('Headingley Stadium', 1187032, 1.2) and ('List of international cricket centuries at Headingley', 6624161, 1.2)\n",
      "worcestershire has too many candidates 180\n",
      "essex has too many candidates 618\n",
      "leicestershire has too many candidates 167\n",
      "surrey has too many candidates 387\n",
      "warwickshire has too many candidates 197\n",
      "australians has too many candidates 164\n",
      "somerset has too many candidates 693\n",
      "oval has too many candidates 211\n",
      "can not decide between ('Paul Johnson (writer)', 45997, 1.4) and ('Paul Johnson (squash player)', 2278396, 1.4)\n",
      "west indian has too many candidates 198\n",
      "can not decide between ('Chris Adams (wrestler)', 860227, 1.4) and ('Chris Adams (footballer)', 5105681, 1.4)\n",
      "yorkshire has too many candidates 1055\n",
      "derbyshire has too many candidates 435\n",
      "england has too many candidates 2693\n",
      "west indian has too many candidates 198\n",
      "can not decide between ('Headingley Stadium', 1187032, 1.4) and ('List of international cricket centuries at Headingley', 6624161, 1.4)\n",
      "derbyshire has too many candidates 435\n",
      "australians has too many candidates 164\n",
      "yorkshire has too many candidates 1055\n",
      "can not decide between ('Paul Johnson (writer)', 45997, 1.4) and ('Paul Johnson (squash player)', 2278396, 1.4)\n",
      "can not decide between ('Phil Simmons', 6827914, 1.4) and ('Phil Simmons (rower)', 47434073, 1.4)\n",
      "can not decide between ('Tom Moody', 3537597, 1.5) and ('Tom Moody (artist)', 7816917, 1.5)\n",
      "england has too many candidates 2693\n",
      "surrey has too many candidates 387\n",
      "leicestershire has too many candidates 167\n",
      "oval has too many candidates 211\n",
      "hussein has too many candidates 384\n",
      "somerset has too many candidates 693\n",
      "nottinghamshire has too many candidates 215\n",
      "essex has too many candidates 618\n",
      "can not decide between ('Chris Adams (wrestler)', 860227, 1.4) and ('Chris Adams (footballer)', 5105681, 1.4)\n",
      "kent has too many candidates 1556\n",
      "london has too many candidates 4659\n",
      "simmons has too many candidates 446\n",
      "worcestershire has too many candidates 180\n",
      "warwickshire has too many candidates 197\n",
      "leicestershire has too many candidates 167\n",
      "can not decide between ('Royal Tunbridge Wells', 665489, 9999) and ('Opera House, Royal Tunbridge Wells', 6463096, 9999)\n",
      "english has too many candidates 3563\n",
      "leicester has too many candidates 353\n",
      "northamptonshire has too many candidates 208\n",
      "surrey has too many candidates 387\n",
      "No match for english county championship\n",
      "glamorgan has too many candidates 125\n",
      "can not decide between ('The Oval (Belfast)', 176401, 9999) and ('The Oval Portrait', 878571, 9999)\n",
      "kent has too many candidates 1556\n",
      "bristol has too many candidates 1019\n",
      "somerset has too many candidates 693\n",
      "gloucestershire has too many candidates 271\n",
      "hove has too many candidates 148\n",
      "can not decide between ('Chester-le-Street (district)', 1070655, 9999) and ('Chester-le-Street', 2570025, 9999)\n",
      "nottinghamshire has too many candidates 215\n",
      "worcestershire has too many candidates 180\n",
      "sussex has too many candidates 477\n",
      "hampshire has too many candidates 1760\n",
      "lancashire has too many candidates 544\n",
      "durham has too many candidates 690\n",
      "warwickshire has too many candidates 197\n",
      "derbyshire has too many candidates 435\n",
      "portsmouth has too many candidates 308\n",
      "london has too many candidates 4659\n",
      "middlesex has too many candidates 218\n",
      "glamorgan has too many candidates 125\n",
      "scotland has too many candidates 2062\n",
      "worcestershire has too many candidates 180\n",
      "leeds has too many candidates 584\n",
      "sussex has too many candidates 477\n",
      "can not decide between ('Trent Bridge', 2096399, 9999) and ('List of international cricket centuries at Trent Bridge', 6624169, 9999)\n",
      "london has too many candidates 4659\n",
      "can not decide between ('Edgbaston Waterworks', 82303, 9999) and ('Edgbaston goby', 306774, 9999)\n",
      "birmingham has too many candidates 907\n",
      "ashes has too many candidates 205\n",
      "durham has too many candidates 690\n",
      "oxford has too many candidates 1102\n",
      "surrey has too many candidates 387\n",
      "nottingham has too many candidates 463\n",
      "leicestershire has too many candidates 167\n",
      "can not decide between ('Headingley Stadium', 1187032, 9999) and ('Headingley', 1592322, 9999)\n",
      "can not decide between ('Old Trafford', 83457, 9999) and ('Old Trafford (district)', 1455478, 9999)\n",
      "manchester has too many candidates 1402\n",
      "somerset has too many candidates 693\n",
      "northampton has too many candidates 252\n",
      "middlesex has too many candidates 218\n",
      "kent has too many candidates 1556\n",
      "gloucestershire has too many candidates 271\n",
      "hampshire has too many candidates 1760\n",
      "australia has too many candidates 7552\n",
      "england has too many candidates 2693\n",
      "can not decide between ('Universities in the United Kingdom', 918342, 9999) and ('List of universities in the United Kingdom', 2746976, 9999)\n",
      "derbyshire has too many candidates 435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id = 1000\n",
    "MAX_CANDIDATES = 50\n",
    "OLD_SIZE = np.inf\n",
    "current_document = []\n",
    "saved_candidates = {}\n",
    "\n",
    "for index in tqdm(range(len(testdf)), desc=\"Processing\", total=len(testdf)):\n",
    "    row = testdf.iloc[index]\n",
    "    current_document.append({\n",
    "        'id': row['id'],\n",
    "        'token': row['token'],\n",
    "        'full_mention': row['en_redirect_title'],\n",
    "        'wiki_url': row['wiki_url'],\n",
    "        'item_id': row['item_id']\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        if row['token'].startswith('-DOCSTART-'):\n",
    "            document_df = pd.DataFrame(current_document)\n",
    "            current_document = []\n",
    "\n",
    "            full_mention_ = document_df[document_df['wiki_url'] != 'NOT_FOUND']\n",
    "            found = full_mention_[full_mention_['wiki_url'] != '?']\n",
    "            item_id_train = set(found.item_id.tolist())\n",
    "            full_mention_found = dict(zip(found['full_mention'].str.lower(), found['wiki_url']))\n",
    "\n",
    "            not_found = full_mention_[full_mention_['wiki_url'] == '?'].copy()\n",
    "            not_found.full_mention = not_found.full_mention.str.lower()\n",
    "            mention_test = set(not_found.full_mention.tolist())\n",
    "\n",
    "            old_size = OLD_SIZE\n",
    "            new_size = len(mention_test)\n",
    "            correct_links = []\n",
    "            for tries in range(3):\n",
    "                if new_size < old_size:\n",
    "                    random_order = list(mention_test)\n",
    "                    random.shuffle(random_order)\n",
    "\n",
    "                    for uncertain_word in (random_order):\n",
    "                        correct_link = None\n",
    "                        matching_urls = set(\n",
    "                            [url for mention, url in full_mention_found.items() if uncertain_word in mention])\n",
    "                        if len(matching_urls) == 1:  # if uncertain_word is a part of the correct entity\n",
    "                            full_mention_found[uncertain_word] = list(matching_urls)[0]\n",
    "                        elif len(matching_urls) > 1:\n",
    "                            print(uncertain_word, \"seems to belong to\", matching_urls)\n",
    "                        else:\n",
    "                            filtered_ls = wiki_item.loc[col_.contains(r'\\b{}\\b'.format(uncertain_word),na=False)] if uncertain_word not in saved_candidates.keys() else \\\n",
    "                            saved_candidates[uncertain_word]\n",
    "                            saved_candidates[uncertain_word] = filtered_ls\n",
    "                            no_candidates = len(filtered_ls)\n",
    "                            if not no_candidates:\n",
    "                                print(\"No match for\", uncertain_word)\n",
    "                            elif no_candidates == 1:\n",
    "                                full_mention_found[uncertain_word] = URL + \\\n",
    "                                                                     filtered_ls.wikipedia_title.tolist()[0].replace(\n",
    "                                                                         ' ', '_')\n",
    "                                item_id_train.add(filtered_ls.item_id.tolist()[0])\n",
    "                            elif no_candidates < MAX_CANDIDATES:\n",
    "                                distances = get_all_dist(filtered_ls, item_id_train)\n",
    "                                if len(distances) > 1:\n",
    "                                    first_candidate, second_candidate = heapq.nsmallest(2, distances,\n",
    "                                                                                        key=lambda x: x[-1])\n",
    "                                    if first_candidate[-1] < second_candidate[-1]:\n",
    "                                        title, choice, _ = first_candidate\n",
    "                                        full_mention_found[\n",
    "                                            uncertain_word] = URL + title.replace(' ', '_')\n",
    "                                        item_id_train.add(choice)\n",
    "                                    else:\n",
    "                                        print(\"can not decide between\", first_candidate, \"and\", second_candidate)\n",
    "                                else:\n",
    "                                    title, choice, _ = distances[0]\n",
    "                                    full_mention_found[\n",
    "                                        uncertain_word] = URL + title.replace(' ', '_')\n",
    "                                    item_id_train.add(choice)\n",
    "\n",
    "                            else:\n",
    "                                print(uncertain_word, \"has too many candidates\", no_candidates)\n",
    "                    old_size = len(mention_test)\n",
    "                    mention_test.difference_update(full_mention_found.keys())\n",
    "                    new_size = len(mention_test)\n",
    "            if len(not_found):\n",
    "                not_found.wiki_url = not_found.full_mention.map(full_mention_found)\n",
    "                not_found.to_csv(f'{CORRECTED_FOLDER}{id}.csv', index=False)\n",
    "                id += 1\n",
    "    except Exception as e:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:13:28.999592400Z",
     "start_time": "2023-12-17T15:11:11.854685Z"
    }
   },
   "id": "763d11b49dbff51a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "partial = result_first_part\n",
    "testdf = test_df[['id', 'token', 'full_mention']]\n",
    "testdf = testdf.merge(partial, on='id')\n",
    "display(testdf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:13:29.001594400Z",
     "start_time": "2023-12-17T15:13:29.001594400Z"
    }
   },
   "id": "2cb917ad6d680791"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrected_df = pd.DataFrame()\n",
    "counter = 0\n",
    "for i in range(100000):\n",
    "    try:\n",
    "        corrected_df = pd.concat([corrected_df, pd.read_csv(f'{CORRECTED_FOLDER}{i}.csv')[['id', 'wiki_url']]],\n",
    "                                 ignore_index=True)\n",
    "        counter += 1\n",
    "    except Exception as e:\n",
    "        pass\n",
    "corrected_df = corrected_df.drop_duplicates('id')\n",
    "counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:13:29.002594300Z",
     "start_time": "2023-12-17T15:13:29.001594400Z"
    }
   },
   "id": "a1b476e3b1144af7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df = pd.merge(testdf, corrected_df, on='id', how='left', suffixes=('_original', '_update'))\n",
    "\n",
    "merged_df['wiki_url'] = merged_df['wiki_url_update'].combine_first(merged_df['wiki_url_original'])\n",
    "\n",
    "merged_df = merged_df.drop(['wiki_url_original', 'wiki_url_update'], axis=1)\n",
    "print(len(merged_df[merged_df.wiki_url == '?']))\n",
    "merged_df.loc[merged_df['wiki_url'] == '?', 'wiki_url'] = 'NOT_FOUND'\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:13:29.003593900Z",
     "start_time": "2023-12-17T15:13:29.002594300Z"
    }
   },
   "id": "70e3fd5e0128c4a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_df[['id', 'wiki_url']].to_csv(SUBMISSIONS_FOLDER + 'submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T15:13:29.005112600Z",
     "start_time": "2023-12-17T15:13:29.004104900Z"
    }
   },
   "id": "f42ed16e4c816fad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
