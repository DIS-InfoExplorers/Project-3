{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project 3. InfoExplorers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e42b070e34804c0e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:32.882562Z",
     "start_time": "2023-12-18T18:27:32.264925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsaikali/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/ada-2023-project-dataexplorers/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import heapq\n",
    "import random\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0751a99dd67d9a7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Constant paths\n",
    "DATA_FOLDER = 'data/'\n",
    "PICKLES_FOLDER = 'pickles/'\n",
    "WIKILITE_FOLDER = DATA_FOLDER + 'wiki_lite/'\n",
    "SUBMISSIONS_FOLDER = 'submissions/'\n",
    "CORRECTED_FOLDER = 'corrected/'\n",
    "\n",
    "# Create folders if they don't exist\n",
    "if not os.path.exists(PICKLES_FOLDER):\n",
    "    os.makedirs(PICKLES_FOLDER)\n",
    "\n",
    "if not os.path.exists(SUBMISSIONS_FOLDER):\n",
    "    os.makedirs(SUBMISSIONS_FOLDER)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:32.888073Z",
     "start_time": "2023-12-18T18:27:32.884018Z"
    }
   },
   "id": "18f1bb4a3fc83cd6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Train data loading\n",
    "try:\n",
    "    train_df = pd.read_pickle(PICKLES_FOLDER + 'train_df.pkl')\n",
    "except:\n",
    "    train_df = pd.read_csv(DATA_FOLDER + 'train.csv')\n",
    "    train_df.to_pickle(PICKLES_FOLDER + 'train_df.pkl')\n",
    "\n",
    "# Test data loading  \n",
    "try:\n",
    "    test_df = pd.read_pickle(PICKLES_FOLDER + 'test_df.pkl')\n",
    "except:\n",
    "    test_df = pd.read_csv(DATA_FOLDER + 'test.csv')\n",
    "    test_df.to_pickle(PICKLES_FOLDER + 'test_df.pkl')\n",
    "\n",
    "# Redirects loading\n",
    "try:\n",
    "    enwinki_redirects = pd.read_pickle(PICKLES_FOLDER + 'enwiki_redirects.pkl')\n",
    "except:\n",
    "    enwinki_redirects = pd.read_csv(WIKILITE_FOLDER + 'enwiki_redirects.tsv', names=['en_title', 'en_redirect_title'],\n",
    "                                    sep='\\t')\n",
    "    enwinki_redirects.to_pickle(PICKLES_FOLDER + 'enwiki_redirects.pkl')\n",
    "\n",
    "# Aliases loading  \n",
    "try:\n",
    "    item_aliases = pd.read_pickle(PICKLES_FOLDER + 'item_aliases.pkl')\n",
    "except:\n",
    "    item_aliases = pd.read_csv(WIKILITE_FOLDER + 'item_aliases.csv')\n",
    "    item_aliases.to_pickle(PICKLES_FOLDER + 'item_aliases.pkl')\n",
    "\n",
    "# Properties loading\n",
    "try:\n",
    "    properties = pd.read_pickle(PICKLES_FOLDER + 'property.pkl')\n",
    "except:\n",
    "    properties = pd.read_csv(WIKILITE_FOLDER + 'property.csv')\n",
    "    properties.to_pickle(PICKLES_FOLDER + 'property.pkl')\n",
    "\n",
    "# Statements loading\n",
    "try:\n",
    "    statements = pd.read_pickle(PICKLES_FOLDER + 'statements.pkl')\n",
    "except:\n",
    "    statements = pd.read_csv(WIKILITE_FOLDER + 'statements.csv')\n",
    "    statements.to_pickle(PICKLES_FOLDER + 'statements.pkl')\n",
    "\n",
    "# Wiki items loading \n",
    "try:\n",
    "    wiki_items = pd.read_pickle(PICKLES_FOLDER + 'wiki_items.pkl')\n",
    "except:\n",
    "    wiki_items = pd.read_csv(WIKILITE_FOLDER + 'wiki_items.csv')\n",
    "    wiki_items.to_pickle(PICKLES_FOLDER + 'wiki_items.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:34.990179Z",
     "start_time": "2023-12-18T18:27:32.889225Z"
    }
   },
   "id": "8b9e6f0ee2813fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Using existing datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74aa5a0e772b9c9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f73f63d272f417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merging `item_aliases` and `wiki_items` on `item_id` to get the `wikipedia_title` for each `en_alias`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b64eefea3ce0942"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   item_id  en_label                                 en_description  \\\n0        1  Universe             totality of space and all contents   \n1        1  Universe             totality of space and all contents   \n2        1  Universe             totality of space and all contents   \n3        1  Universe             totality of space and all contents   \n4        2     Earth  third planet from the Sun in the Solar System   \n\n  wikipedia_title      en_alias  \n0        Universe  Our Universe  \n1        Universe  The Universe  \n2        Universe    The Cosmos  \n3        Universe        cosmos  \n4           Earth   Blue Planet  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item_id</th>\n      <th>en_label</th>\n      <th>en_description</th>\n      <th>wikipedia_title</th>\n      <th>en_alias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>Our Universe</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>The Universe</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>The Cosmos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Universe</td>\n      <td>totality of space and all contents</td>\n      <td>Universe</td>\n      <td>cosmos</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Earth</td>\n      <td>third planet from the Sun in the Solar System</td>\n      <td>Earth</td>\n      <td>Blue Planet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_wiki_items = wiki_items.merge(item_aliases, how='left', on='item_id')\n",
    "merged_wiki_items.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:35.852527Z",
     "start_time": "2023-12-18T18:27:34.991886Z"
    }
   },
   "id": "bd280c1982f55afb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Copying the dataframes to modify them\n",
    "test_df_mod = test_df.copy(deep=True)\n",
    "train_df_mod = train_df.copy(deep=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:35.860713Z",
     "start_time": "2023-12-18T18:27:35.852725Z"
    }
   },
   "id": "82965c83ebbb3015"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Beginning of the URL to wikipedia\n",
    "URL = 'http://en.wikipedia.org/wiki/'\n",
    "LEN_URL = len(URL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:35.865841Z",
     "start_time": "2023-12-18T18:27:35.860988Z"
    }
   },
   "id": "2cb2450d10518b95"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# We need to lowercase the tokens to avoid problems with case in future steps\n",
    "train_df_mod['full_mention_lower'] = train_df_mod['full_mention'].str.lower()\n",
    "\n",
    "# We only keep the tokens that have a wiki_url\n",
    "train_df_mod = train_df_mod[train_df_mod['wiki_url'].notnull() & (train_df_mod['wiki_url'] != '--NME--')]\n",
    "\n",
    "# We also lowercase the tokens in the test data for future steps\n",
    "test_df_mod['full_mention'] = test_df_mod['full_mention'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:35.917687Z",
     "start_time": "2023-12-18T18:27:35.872772Z"
    }
   },
   "id": "7249692acea2e0eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We transform:\n",
    "- `en_alias` and `wikipedia_title` in `merged_wiki_items`\n",
    "- `wikipedia_title` in `wiki_items`\n",
    "- `en_title` in `redirects`\n",
    "\n",
    "to lowercase to avoid problems with case in future steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd5f505f0444795b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "merged_wiki_items['en_alias_lower'] = merged_wiki_items['en_alias'].str.lower()\n",
    "merged_wiki_items['wikipedia_title_lower'] = merged_wiki_items['wikipedia_title'].str.lower()\n",
    "\n",
    "wiki_items['wikipedia_title_lower'] = wiki_items['wikipedia_title'].str.lower()\n",
    "\n",
    "enwinki_redirects['en_title_lower'] = enwinki_redirects['en_title'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:38.129874Z",
     "start_time": "2023-12-18T18:27:35.999128Z"
    }
   },
   "id": "7d96a359b740d910"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating dictionaries for faster lookup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef8e3b286066879e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "DICT_FOLDER = PICKLES_FOLDER + 'dictionaries/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:38.136590Z",
     "start_time": "2023-12-18T18:27:38.130273Z"
    }
   },
   "id": "962b2fae4cacf401"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "try:\n",
    "    aliases_dict = pickle.load(open(DICT_FOLDER + 'aliases_dict.pkl', 'rb'))\n",
    "except:\n",
    "    aliases = merged_wiki_items[['en_alias_lower', 'wikipedia_title']].groupby(['en_alias_lower']).agg(lambda x: x.tolist())\n",
    "    aliases_dict = pd.Series(aliases['wikipedia_title'].values, index=aliases.index).to_dict()\n",
    "    \n",
    "    pickle.dump(aliases_dict, open(DICT_FOLDER + 'aliases_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:39.157611Z",
     "start_time": "2023-12-18T18:27:38.136517Z"
    }
   },
   "id": "2712dc3b67f3fd2d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "try:\n",
    "    titles_dict = pickle.load(open(DICT_FOLDER + 'titles_dict.pkl', 'rb'))\n",
    "except:\n",
    "    titles = wiki_items[['wikipedia_title_lower', 'wikipedia_title']].groupby(['wikipedia_title_lower']).agg(lambda x: x.tolist())\n",
    "    titles_dict = pd.Series(titles['wikipedia_title'].values, index=titles.index).to_dict()\n",
    "    \n",
    "    pickle.dump(titles_dict, open(DICT_FOLDER + 'titles_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:42.333833Z",
     "start_time": "2023-12-18T18:27:39.158604Z"
    }
   },
   "id": "168feba5ec999738"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dict = pickle.load(open(DICT_FOLDER + 'train_dict.pkl', 'rb'))\n",
    "except:\n",
    "    train_dict = pd.Series(train_df_mod['wiki_url'].values, index=train_df_mod['full_mention_lower']).to_dict()\n",
    "    pickle.dump(train_dict, open(DICT_FOLDER + 'train_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:42.339837Z",
     "start_time": "2023-12-18T18:27:42.334559Z"
    }
   },
   "id": "70b4e1a8a4230832"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "try:\n",
    "    redirects_dict = pickle.load(open(DICT_FOLDER + 'redirects_dict.pkl', 'rb'))\n",
    "except:\n",
    "    redirects_dict = pd.Series(enwinki_redirects['en_redirect_title'].values,\n",
    "                               index=enwinki_redirects['en_title_lower']).to_dict()\n",
    "    pickle.dump(redirects_dict, open(DICT_FOLDER + 'redirects_dict.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:44.369586Z",
     "start_time": "2023-12-18T18:27:42.339538Z"
    }
   },
   "id": "846f1615b68022c1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104890/104890 [00:01<00:00, 64066.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# We want to keep the count of the number of matches we find with aliases and redirects\n",
    "aliases_matching = 0\n",
    "redirects_matching = 0\n",
    "\n",
    "for index, row in tqdm(test_df_mod.iterrows(), total=test_df_mod.shape[0]):\n",
    "    if str(row['wiki_url']) == 'nan' or row['wiki_url'] != '?':\n",
    "        continue\n",
    "\n",
    "    token = row['full_mention']\n",
    "    train_url = train_dict.get(token)\n",
    "\n",
    "    if train_url is not None:\n",
    "        # We found a link in the train data and it is the true identity so we can use it\n",
    "        test_df_mod.at[index, 'wiki_url'] = train_url\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        wiki_title = None\n",
    "        \n",
    "        wiki_titles = titles_dict.get(token)\n",
    "\n",
    "        if wiki_titles is not None:\n",
    "            if len(wiki_titles) == 1:\n",
    "                wiki_title = wiki_titles[0]\n",
    "                aliases_matching += 1\n",
    "\n",
    "        else:\n",
    "            wiki_titles = aliases_dict.get(token)\n",
    "\n",
    "            if wiki_titles is not None:\n",
    "                if len(wiki_titles) == 1:\n",
    "                    wiki_title = wiki_titles[0]\n",
    "                    aliases_matching += 1\n",
    "\n",
    "    if wiki_title is not None:\n",
    "        redirect_title = redirects_dict.get(wiki_title.lower())\n",
    "\n",
    "        if redirect_title is not None:\n",
    "            redirects_matching += 1\n",
    "            test_df_mod.at[index, 'wiki_url'] = URL + redirect_title.replace(' ', '_')\n",
    "\n",
    "        else:\n",
    "            test_df_mod.at[index, 'wiki_url'] = URL + wiki_title.replace(' ', '_')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.059675Z",
     "start_time": "2023-12-18T18:27:44.371666Z"
    }
   },
   "id": "f4c8cc92465b418d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used 1919 entity aliases for matching urls\n"
     ]
    }
   ],
   "source": [
    "print(f\"We used {aliases_matching} entity aliases for matching urls\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.066704Z",
     "start_time": "2023-12-18T18:27:46.060359Z"
    }
   },
   "id": "416956ed3cbe7456"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used 380 link redirections for matching urls\n"
     ]
    }
   ],
   "source": [
    "print(f\"We used {redirects_matching} link redirections for matching urls\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.066956Z",
     "start_time": "2023-12-18T18:27:46.062772Z"
    }
   },
   "id": "38d44acc9170b212"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how many tokens we found links for:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7507b3a85b5a8586"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously we had 9166 tokens without a link\n",
      "Now we have 735 tokens without a link\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previously we had {test_df[test_df['wiki_url'] == '?']['wiki_url'].count()} tokens without a link\")\n",
    "print(f\"Now we have {test_df_mod[test_df_mod['wiki_url'] == '?']['wiki_url'].count()} tokens without a link\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.073750Z",
     "start_time": "2023-12-18T18:27:46.067186Z"
    }
   },
   "id": "c3e735d9fe28d308"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sanitize First part result for second part"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2f5746f1b4d3123"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "result_first_part = test_df_mod[['id', 'wiki_url']]\n",
    "result_first_part.loc[:, 'wiki_url'] = result_first_part['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http') or str(x) == '?') else x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.107399Z",
     "start_time": "2023-12-18T18:27:46.074509Z"
    }
   },
   "id": "df485ff6287f478b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to submit only the first part, run the below code to create the necessary csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc812b18587977ae"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# result_first_part.loc[:, 'wiki_url'] = result_first_part['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http')) else x)\n",
    "# name = 'tr_title_alias_dict_redirects_url'\n",
    "# result_first_part.to_csv(SUBMISSIONS_FOLDER + name + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.107539Z",
     "start_time": "2023-12-18T18:27:46.104764Z"
    }
   },
   "id": "d0f8b28d751a7ce8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Knowledge Graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f374ebae8b86d158"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def add_edges(G, connection, progress=True):\n",
    "    \"\"\"\n",
    "    Function adding the edges to the graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G: nx.Graph\n",
    "        Graph to which the edges are added.\n",
    "    connection: pd.DataFrame\n",
    "        DataFrame containing the connections between the wiki_items.\n",
    "    progress: bool, optional (default=True)\n",
    "        If True, a progress bar is shown.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    G: nx.Graph\n",
    "        Graph with the added edges. \n",
    "    \"\"\"\n",
    "    \n",
    "    if progress:\n",
    "        for source_item_id, _, target_item_id in tqdm(connection.iloc, total=len(connection)):\n",
    "            G.add_edge(source_item_id, target_item_id)\n",
    "    else:\n",
    "        G = nx.from_pandas_edgelist(connection, 'source_item_id', 'target_item_id', create_using=nx.Graph)\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.114786Z",
     "start_time": "2023-12-18T18:27:46.108638Z"
    }
   },
   "id": "e80f9d6941dae5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "PICKLE_FILENAME = \"graph_undirected_full.pkl\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:46.114940Z",
     "start_time": "2023-12-18T18:27:46.110854Z"
    }
   },
   "id": "4e5f0950aac4c8de"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the graph from the pickle file\n",
    "    with open(PICKLES_FOLDER + PICKLE_FILENAME, 'rb') as pickle_file:\n",
    "        G = pickle.load(pickle_file)\n",
    "except:\n",
    "    G = nx.Graph()  # Undirected Graph\n",
    "    G = add_edges(G, statements, progress=True)\n",
    "    \n",
    "    # Save graph pickle to the file\n",
    "    with open(PICKLES_FOLDER + PICKLE_FILENAME, 'wb') as pickle_file:\n",
    "        pickle.dump(G, pickle_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:54.610158Z",
     "start_time": "2023-12-18T18:27:46.113822Z"
    }
   },
   "id": "6d20743712452f8e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4906271\n",
      "Number of edges: 24528246\n"
     ]
    }
   ],
   "source": [
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:55.462576Z",
     "start_time": "2023-12-18T18:27:54.642068Z"
    }
   },
   "id": "789a31308f2cfcff"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN             NaN   \n1            1                        CRICKET        NaN             NaN   \n2            2                              -        NaN             NaN   \n3            3                 LEICESTERSHIRE          B  LEICESTERSHIRE   \n4            4                           TAKE        NaN             NaN   \n...        ...                            ...        ...             ...   \n104885  104885                        brother        NaN             NaN   \n104886  104886                              ,        NaN             NaN   \n104887  104887                          Bobby          B           Bobby   \n104888  104888                              .        NaN             NaN   \n104889  104889                            NaN        NaN             NaN   \n\n                                                 wiki_url  \n0                                               NOT_FOUND  \n1                                               NOT_FOUND  \n2                                               NOT_FOUND  \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...  \n4                                               NOT_FOUND  \n...                                                   ...  \n104885                                          NOT_FOUND  \n104886                                          NOT_FOUND  \n104887                 http://en.wikipedia.org/wiki/Bobby  \n104888                                          NOT_FOUND  \n104889                                          NOT_FOUND  \n\n[104890 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>LEICESTERSHIRE</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>Bobby</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking the result from the first part\n",
    "test_df.update(result_first_part)\n",
    "display(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:55.474855Z",
     "start_time": "2023-12-18T18:27:55.462841Z"
    }
   },
   "id": "c74cc83c41c2746b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetch item_ids for each entity:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103c4246faf2f324"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "wiki_item = wiki_items[['item_id', 'en_label', 'wikipedia_title']]\n",
    "lower_case_wiki_item_titles = wiki_item.wikipedia_title.str.lower().str\n",
    "\n",
    "test_df['wikipedia_title'] = test_df.wiki_url.str[LEN_URL:].str.replace('_', ' ')\n",
    "test_df = test_df.merge(wiki_item, on='wikipedia_title', how='left').drop(\n",
    "    columns=['wikipedia_title', 'en_label'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:57.990523Z",
     "start_time": "2023-12-18T18:27:55.481983Z"
    }
   },
   "id": "caf97f2b1b17be22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions \n",
    "\n",
    "#### Distance functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e6479c2ad97242c"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def get_dist(doc_assigned_ids, candidate_id, fill_na=9999):\n",
    "    \"\"\"\n",
    "    Function calculating the distance between the full mention and a candidate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_assigned_ids: list\n",
    "        List containing the already assigned entities.\n",
    "    candidate_id: float\n",
    "        Candidate id.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distance: float\n",
    "        Average distance between the full mention and the candidate in the graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance_list = []\n",
    "    \n",
    "    #Retrieve only a subset of ids of the document\n",
    "    subset_certain = list(doc_assigned_ids)\n",
    "    random.shuffle(subset_certain)\n",
    "    subset_size = 10\n",
    "    subset_certain = subset_certain[:subset_size]\n",
    "    \n",
    "    for assigned_entity in subset_certain:\n",
    "        try:\n",
    "            shortest = nx.shortest_path_length(G, source=assigned_entity, target=candidate_id)\n",
    "            distance_list.append(shortest)\n",
    "        except:\n",
    "            distance_list.append(fill_na)\n",
    "            \n",
    "    return sum(distance_list) / subset_size if distance_list else fill_na"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:57.996346Z",
     "start_time": "2023-12-18T18:27:57.991556Z"
    }
   },
   "id": "a769a09c2239499"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_all_dist(candidates, doc_assigned_ids):\n",
    "    \"\"\"\n",
    "    Function calculating the distance between the full mention and all the candidates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    candidates: pd.DataFrame\n",
    "        DataFrame containing the candidates.\n",
    "    doc_assigned_ids: list\n",
    "        List of the already assigned entities.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    distances: list\n",
    "        List containing the distances between the full mention and all the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for candidate_id, _, candidate_title in candidates.iloc:\n",
    "        distances.append((candidate_title, candidate_id, get_dist(doc_assigned_ids, candidate_id)))\n",
    "        \n",
    "    return distances"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:57.996489Z",
     "start_time": "2023-12-18T18:27:57.994304Z"
    }
   },
   "id": "b3d5f1e3fa796f46"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def find_best_candidate(candidate_df, found_ids):\n",
    "    \"\"\"\n",
    "    Function returning the best candidate for the mention.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    candidate_df: pd.DataFrame\n",
    "        dataframe containing all url candidates for a mention\n",
    "    found_ids: pd.Serie\n",
    "        serie containing all ids of a given document\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_candidate_item_id: int\n",
    "        candidate item_id closest to given ids.\n",
    "    best_candidate_url: str\n",
    "        candidate wikipedia url which is closest to the given ids.\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = get_all_dist(candidate_df, found_ids)\n",
    "    obtained_distances = len(distances)\n",
    "    \n",
    "    # One distance is obtained, it must be the only possible matching => use it.\n",
    "    if obtained_distances == 1:\n",
    "        title, item_id, _ = distances[0]\n",
    "        return item_id, title_to_wiki_url(title)\n",
    "    \n",
    "    # More than one distance is obtained, multiple matching exists.\n",
    "    elif obtained_distances >=1:\n",
    "        first_candidate, second_candidate = heapq.nsmallest(2, distances,key=lambda x: x[-1])  \n",
    "        \n",
    "        # If similarity of the first candidate and second candidate is not the same, take the first.\n",
    "        # Otherwise, we are unsure and return NaNs.\n",
    "        if first_candidate[-1] < second_candidate[-1]:\n",
    "            title, item_id, _ = first_candidate\n",
    "            return item_id, title_to_wiki_url(title)\n",
    "    return np.nan, np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.002264Z",
     "start_time": "2023-12-18T18:27:58.000644Z"
    }
   },
   "id": "a59b4f0f888fcbfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### String manipulation functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e5fffbfb6f56d8a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def find_equal_string_candidates(mention, full_mention_found) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function retrieving list of url for the given mention using the possible of the same document.\n",
    "    Parameters\n",
    "    -------\n",
    "    mention: str\n",
    "        current mention for which we try to find a url in a given document\n",
    "    full_mention_found: dict \n",
    "        dictionary consisting of mentions and their found links in the same document\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    candidate_urls: list\n",
    "        candidate urls option for the given mention.\n",
    "        \n",
    "    -------\n",
    "    Example\n",
    "        Let's say the document references Roger Federer once, and it is already mapped to the link https://en.wikipedia.org/wiki/Roger_Federer.\n",
    "        This function allows finding any entity starting with \"Roger\" or \"Federer\" and map it to this link https://en.wikipedia.org/wiki/Roger_Federer.\n",
    "    \"\"\"\n",
    "    #TODO doc.\n",
    "    candidate_urls = set()\n",
    "    for found_mention, url in full_mention_found.items():\n",
    "        if mention in found_mention:\n",
    "            candidate_urls.add(url)\n",
    "    return  list(candidate_urls)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.008028Z",
     "start_time": "2023-12-18T18:27:58.003502Z"
    }
   },
   "id": "42657415b7e35c00"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def find_substring_candidates(mention, saved_candidates):\n",
    "    \"\"\"\n",
    "    Function retrieving list of possible candidates for the given mention thanks to the wiki_items dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    mention: str\n",
    "        current mention for which we try to find a url in a given document\n",
    "    saved_candidates: dict \n",
    "        dictionary consisting of already found mappings to candidates\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    candidates: pd.DataFrame\n",
    "        candidates wikipedia titles option for the given mention.\n",
    "        \n",
    "    -------\n",
    "    Example\n",
    "        Let's say the document references Roger once, and we don't know which Roger it is\n",
    "        This function allows finding any entity containing the word \"Roger\", and return their respective attributes (Roger Frederer, Roger Moore, Roger Waters ...)\n",
    "    \"\"\"\n",
    "    if mention not in saved_candidates.keys(): \n",
    "        candidates = wiki_item.loc[lower_case_wiki_item_titles.contains(r'\\b{}\\b'.format(mention), na=False)]\n",
    "        saved_candidates[mention] = candidates\n",
    "        return candidates\n",
    "    else:\n",
    "        return saved_candidates[mention]  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.008159Z",
     "start_time": "2023-12-18T18:27:58.006422Z"
    }
   },
   "id": "5a7fb74273865f75"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def title_to_wiki_url(title):\n",
    "    \"\"\"\n",
    "    Function transforming a wikipedia title into a wikipedia url\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    title: str\n",
    "        wikipedia title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    wikipedia_url: str\n",
    "        wikipedia url\n",
    "    \"\"\"\n",
    "    return URL + title.replace(' ', '_')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.013728Z",
     "start_time": "2023-12-18T18:27:58.008760Z"
    }
   },
   "id": "93c18a966273d5ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Document manipulation functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d78f7a1038c64b2"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def find_doc_range(df):\n",
    "    \"\"\"\n",
    "    Function returning a zipped tuple of end indexes and start indexes of documents.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame containing all documents.  \n",
    "    Returns\n",
    "    -------\n",
    "    docs_range: zip\n",
    "        zipped document range between start ids and ending ids of each document.\n",
    "    \"\"\"\n",
    "    def check_docstart(row):\n",
    "        \"\"\"\n",
    "        True if the current row is the row where the document starts.\n",
    "        \"\"\"\n",
    "        if pd.notnull(row['token']) and 'DOCSTART' in row['token']:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    data = df.copy()\n",
    "    data['docstart_id'] = df.apply(check_docstart, axis=1)\n",
    "    start_ids = data[data['docstart_id']]['id'].values\n",
    "    end_ids = start_ids[1:] - 1\n",
    "    end_ids = np.append(end_ids,len(df))\n",
    "    docs_range = zip(start_ids, end_ids)\n",
    "    docs_number = len(start_ids)\n",
    "    return docs_range, docs_number"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.023532Z",
     "start_time": "2023-12-18T18:27:58.011641Z"
    }
   },
   "id": "cf7488de0dbb063"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def split_document_findings(document_dataframe):\n",
    "    \"\"\"\n",
    "    Function retrieving and separating entities depending on weather they are associated with a found url or not.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    document_dataframe: pd.DataFrame \n",
    "        document containing all the sentences and entities to match\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    document:  pd.DataFrame\n",
    "        document containing all the sentences and entities to match \n",
    "    found_links: pd.DataFrame\n",
    "        entities that have been already mapped to links by the previous part of the algorithm\n",
    "    not_found_links: pd.DataFrame \n",
    "        entities that are not yet mapped to links\n",
    "    \"\"\"\n",
    "    document = document_dataframe[document_dataframe['wiki_url'] != 'NOT_FOUND']\n",
    "    found_links = document[document['wiki_url'] != '?']\n",
    "    not_found_links = document[document['wiki_url'] == '?']\n",
    "    return document, found_links, not_found_links"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.024707Z",
     "start_time": "2023-12-18T18:27:58.014717Z"
    }
   },
   "id": "61e965a4d1131e68"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def find_within_doc_similarity(found_links, not_found_links):\n",
    "    \"\"\"\n",
    "    This function attempts to find potential links for exact matches within a document.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    found_links: pd.DataFrame\n",
    "        A dataframe of found links.\n",
    "    not_found_links: pd.DataFrame\n",
    "        A dataframe of links that were not found.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    updated_found_links: pd.DataFrame\n",
    "        The updated dataframe of found links.\n",
    "    updated_not_found_links: pd.DataFrame\n",
    "        The updated dataframe of links that were not found.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    Let's say the document references Roger Federer once, and it is already mapped to the link https://en.wikipedia.org/wiki/Roger_Federer.\n",
    "    This function allows finding any entity starting with \"Roger\" or \"Federer\" and map it to this link https://en.wikipedia.org/wiki/Roger_Federer.\n",
    "    \"\"\"  \n",
    "    #Series of list of string \n",
    "    candidates_urls = not_found_links['full_mention'].apply(lambda mention: find_equal_string_candidates(mention, dict(zip(found_links['full_mention'], found_links['wiki_url']))))\n",
    "    \n",
    "    len_candidates_urls = candidates_urls.apply(len)\n",
    "            \n",
    "    #for single candidates, retrieve the value and attribute it.\n",
    "    single_candidates = not_found_links[len_candidates_urls == 1]\n",
    "    single_candidates['wiki_url'] = candidates_urls[len_candidates_urls == 1].apply(lambda l: l[0])\n",
    "    \n",
    "    new_found_links = pd.concat([found_links, single_candidates])\n",
    "    new_not_found_links = not_found_links[len_candidates_urls != 1]\n",
    "\n",
    "    return new_found_links, new_not_found_links"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.024788Z",
     "start_time": "2023-12-18T18:27:58.017995Z"
    }
   },
   "id": "8565d00bf04b5174"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def single_row_df_process(single_row_df):\n",
    "    \"\"\"\n",
    "    Helper function used to retrieve the unique row from a dataframe and process it\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    single_row_df: pd.DataFrame\n",
    "        dataframe with a single row.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    row:\n",
    "        updated row with corrected wiki_url\n",
    "    \"\"\"\n",
    "    row = single_row_df.iloc[0]\n",
    "    row['wiki_url'] = title_to_wiki_url(row['wikipedia_title'])\n",
    "    return row[['item_id', 'wiki_url']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.024828Z",
     "start_time": "2023-12-18T18:27:58.020421Z"
    }
   },
   "id": "36dadb9354188fe5"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def find_substring_similarity(found_links, not_found_links):\n",
    "    \"\"\"\n",
    "    Function retrieving selecting the best candidates for each entity with no link found using the wiki_items dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    found_links: pd.DataFrame\n",
    "        A dataframe of found links.\n",
    "    not_found_links: pd.DataFrame\n",
    "        A dataframe of links that were not found.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    updated_found_links: pd.DataFrame\n",
    "        The updated dataframe of found links.\n",
    "    updated_not_found_links: pd.DataFrame\n",
    "        The updated dataframe of links that were not found.\n",
    "    \"\"\"\n",
    "    #Find all possible candidates from wiki_item dataframe. \n",
    "    #candidates_df is a series of dataframe. type(candidates_df.iloc[0]) = Dataframe\n",
    "    candidates_df = not_found_links['full_mention'].apply(lambda mention: find_substring_candidates(mention, saved_candidates))\n",
    "        \n",
    "    # Calculate the number of candidates for each entity\n",
    "    df_len_per_candidates = candidates_df.apply(len)\n",
    "    \n",
    "    #PHASE ONE : Single possibility elimination.\n",
    "    # If the wiki items have only a single candidate to propose, simply map it.\n",
    "    single_candidates = not_found_links[df_len_per_candidates == 1]\n",
    "\n",
    "    candidates_df_with_one_row = candidates_df[df_len_per_candidates == 1]\n",
    "    if len(candidates_df_with_one_row) != 0:\n",
    "        single_candidates['item_id'], single_candidates['wiki_url'] = candidates_df_with_one_row.apply(single_row_df_process)\n",
    "        \n",
    "    # Update the link-entity maps.\n",
    "    new_found_links = pd.concat([found_links, single_candidates])\n",
    "    new_not_found_links = not_found_links[df_len_per_candidates != 1]\n",
    "    \n",
    "    #PHASE TWO : Multiple possibility elimination.\n",
    "    # Filter for multi candidates, due to time remove entities with too many candidates.\n",
    "    multi_candidates = not_found_links[(df_len_per_candidates > 1) & (df_len_per_candidates <= MAX_CANDIDATES)]\n",
    "    multi_candidates_df = candidates_df[(df_len_per_candidates > 1) & (df_len_per_candidates <= MAX_CANDIDATES)]\n",
    "    \n",
    "    # Using the graph, find the best candidates for each dataframe or NaN if not findable.\n",
    "    item_url_tuples = multi_candidates_df.apply(lambda df: find_best_candidate(df, new_found_links[~new_found_links['item_id'].isna()]['item_id']))\n",
    "    if len(item_url_tuples) !=0:\n",
    "        multi_candidates [['item_id', 'wiki_url']] = multi_candidates.index.to_series().map(item_url_tuples).apply(pd.Series)\n",
    "    \n",
    "    # Update the link-entity maps.  \n",
    "    new_not_found_links = new_not_found_links[~new_not_found_links['id'].isin(new_found_links['id'])]\n",
    "    new_found_links = pd.concat([found_links, multi_candidates])\n",
    "    \n",
    "    return new_found_links, new_not_found_links"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.027632Z",
     "start_time": "2023-12-18T18:27:58.024865Z"
    }
   },
   "id": "a7799683a13c456b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the global constants and variables used for graph decisions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe1e6718735fce5"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "MAX_CANDIDATES = 50\n",
    "MAX_FETCHING_TRIES = 3\n",
    "SEED = 42\n",
    "OLD_SIZE = np.inf\n",
    "saved_candidates = {}\n",
    "\n",
    "random.seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.028548Z",
     "start_time": "2023-12-18T18:27:58.027136Z"
    }
   },
   "id": "e5b788841932e0bb"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "test_df['full_mention'] = test_df['full_mention'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:27:58.037199Z",
     "start_time": "2023-12-18T18:27:58.034804Z"
    }
   },
   "id": "2f44dbbdceb14d4d"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [08:32,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action=\"ignore\")\n",
    "docs_range, docs_count = find_doc_range(test_df)\n",
    "\n",
    "for start_id, end_id in tqdm(docs_range, total=docs_count):\n",
    "    \n",
    "    current_document = test_df.iloc[start_id:end_id]\n",
    "    \n",
    "    # We are interested in the rows that should have a link\n",
    "    current_document, found_links, not_found_links = split_document_findings(current_document)\n",
    "    if len(not_found_links) == 0:\n",
    "        continue\n",
    "      \n",
    "    if len(found_links) == 0:\n",
    "        continue\n",
    "\n",
    "    found_links, not_found_links = find_within_doc_similarity(found_links, not_found_links)\n",
    "    if len(not_found_links) == 0:\n",
    "        test_df.update(found_links)\n",
    "        continue\n",
    "        \n",
    "    if len(found_links) == 0:\n",
    "        test_df.update(found_links)\n",
    "        continue\n",
    "        \n",
    "    found_links, not_found_links = find_substring_similarity(found_links, not_found_links)\n",
    "    test_df.update(found_links)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:36:31.051386Z",
     "start_time": "2023-12-18T18:27:58.037952Z"
    }
   },
   "id": "d7d41bff18135424"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "            id                          token entity_tag    full_mention  \\\n0            0  -DOCSTART- (947testa CRICKET)        NaN             NaN   \n1            1                        CRICKET        NaN             NaN   \n2            2                              -        NaN             NaN   \n3            3                 LEICESTERSHIRE          B  leicestershire   \n4            4                           TAKE        NaN             NaN   \n...        ...                            ...        ...             ...   \n104885  104885                        brother        NaN             NaN   \n104886  104886                              ,        NaN             NaN   \n104887  104887                          Bobby          B           bobby   \n104888  104888                              .        NaN             NaN   \n104889  104889                            NaN        NaN             NaN   \n\n                                                 wiki_url    item_id  \n0                                               NOT_FOUND        NaN  \n1                                               NOT_FOUND        NaN  \n2                                               NOT_FOUND        NaN  \n3       http://en.wikipedia.org/wiki/Leicestershire_Co...  3229147.0  \n4                                               NOT_FOUND        NaN  \n...                                                   ...        ...  \n104885                                          NOT_FOUND        NaN  \n104886                                          NOT_FOUND        NaN  \n104887                 http://en.wikipedia.org/wiki/Bobby   289262.0  \n104888                                          NOT_FOUND        NaN  \n104889                                          NOT_FOUND        NaN  \n\n[104890 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>token</th>\n      <th>entity_tag</th>\n      <th>full_mention</th>\n      <th>wiki_url</th>\n      <th>item_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-DOCSTART- (947testa CRICKET)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>CRICKET</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>LEICESTERSHIRE</td>\n      <td>B</td>\n      <td>leicestershire</td>\n      <td>http://en.wikipedia.org/wiki/Leicestershire_Co...</td>\n      <td>3229147.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>TAKE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>104885</th>\n      <td>104885</td>\n      <td>brother</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104886</th>\n      <td>104886</td>\n      <td>,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104887</th>\n      <td>104887</td>\n      <td>Bobby</td>\n      <td>B</td>\n      <td>bobby</td>\n      <td>http://en.wikipedia.org/wiki/Bobby</td>\n      <td>289262.0</td>\n    </tr>\n    <tr>\n      <th>104888</th>\n      <td>104888</td>\n      <td>.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>104889</th>\n      <td>104889</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NOT_FOUND</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>104890 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:36:31.058567Z",
     "start_time": "2023-12-18T18:36:31.055805Z"
    }
   },
   "id": "e8d39125e83f7134"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "test_df.loc[:, 'wiki_url'] = test_df['wiki_url'].apply(lambda x: 'NOT_FOUND' if not (str(x).startswith('http')) else x)\n",
    "\n",
    "name = 'submission_new_3'\n",
    "test_df[['id', 'wiki_url']].to_csv(name + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T18:36:47.737047Z",
     "start_time": "2023-12-18T18:36:47.659216Z"
    }
   },
   "id": "bb82fc2a2d530089"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf62a0fedb5bbabb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
